{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dog vs cat\\\\train\\\\dog\\\\'\n",
    "import cv2\n",
    "X_train  =[]\n",
    "images = []\n",
    "for i in range(1,100):\n",
    "    #exact = cv2.imread(path+str(i)+'.jpg',0)\n",
    "    exact = path+str(i)+'.jpg'\n",
    "    X_train.append(cv2.resize(cv2.imread(exact, 0), (28, 28)))\n",
    "    #cv2.resize(exact,(28,28))\n",
    "    images.append(exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to numpy array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 28, 28)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining image parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input image dimensions\n",
    "#Large images take too much time and resources.\n",
    "img_rows = 200\n",
    "img_cols = 200\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n",
    "\n",
    "#Define your generator network \n",
    "#Here we are only using Dense layers. But network can be complicated based\n",
    "#on the application. For example, you can use VGG for super res. GAN.         \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)    #Generated image\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "#Alpha — α is a hyperparameter which controls the underlying value to which the\n",
    "#function saturates negatives network inputs.\n",
    "#Momentum — Speed up the training\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "\n",
    "#Given an input image, the Discriminator outputs the likelihood of the image being real.\n",
    "    #Binary classification - true or false (we're calling it validity)\n",
    "\n",
    "def build_discriminator():\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)\n",
    "#The validity is the Discriminator’s guess of input being real or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitting them against each other in model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have constructed our two models it’s time to pit them against each other.\n",
    "#We do this by defining a training function, loading the data set, re-scaling our training\n",
    "#images and setting the ground truths. \n",
    "\n",
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    \n",
    "    #(X_train, _), (_, _) = mnist.load_data()\n",
    "    path = 'dog vs cat\\\\train\\\\dog\\\\'\n",
    "    import cv2\n",
    "    X_train  =[]\n",
    "    images = []\n",
    "    \n",
    "    for i in range(1,100):\n",
    "        #exact = cv2.imread(path+str(i)+'.jpg',0)\n",
    "        exact = path+str(i)+'.jpg'\n",
    "        X_train.append(cv2.resize(cv2.imread(exact, 0), (200, 200)))\n",
    "        images.append(exact)\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    # Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    \n",
    "    \n",
    "    #get images to same size and grayscale\n",
    "\n",
    "#Add channels dimension. As the input to our gen and discr. has a shape 28x28x1.\n",
    "    X_train = np.expand_dims(X_train, axis=3) \n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "\n",
    "#We then loop through a number of epochs to train our Discriminator by first selecting\n",
    "#a random batch of images from our true dataset, generating a set of images from our\n",
    "#Generator, feeding both set of images into our Discriminator, and finally setting the\n",
    "#loss parameters for both the real and fake images, as well as the combined loss. \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half batch of real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    " \n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "        # Generate a half batch of fake images\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator on real and fake images, separately\n",
    "        #Research showed that separate training is more effective. \n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "    #take average loss from real and fake images. \n",
    "    #\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) \n",
    "\n",
    "#And within the same loop we train our Generator, by setting the input noise and\n",
    "#ultimately training the Generator to have the Discriminator label its samples as valid\n",
    "#by specifying the gradient loss.\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "#Create noise vectors as input for generator. \n",
    "#Create as many noise vectors as defined by the batch size. \n",
    "#Based on normal distribution. Output will be of size (batch size, 100)\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100)) \n",
    "\n",
    "        # The generator wants the discriminator to label the generated samples\n",
    "        # as valid (ones)\n",
    "        #This is where the genrator is trying to trick discriminator into believing\n",
    "        #the generated image is true (hence value of 1 for y)\n",
    "        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
    "\n",
    "        # Generator is part of combined where it got directly linked with the discriminator\n",
    "        # Train the generator with noise as x and 1 as y. \n",
    "        # Again, 1 as the output as it is adversarial and if generator did a great\n",
    "        #job of folling the discriminator then the output would be 1 (true)\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "\n",
    "#Additionally, in order for us to keep track of our training process, we print the\n",
    "#progress and save the sample image output depending on the epoch interval specified.  \n",
    "# Plot the progress\n",
    "        \n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "#when the specific sample_interval is hit, we call the\n",
    "#sample_image function. Which looks as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/dog_%d.png\" % epoch)\n",
    "    plt.close()\n",
    "#This function saves our images for us to view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and compiling the 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 512)               20480512  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 20,612,097\n",
      "Trainable params: 20,612,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_74 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 40000)             41000000  \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 200, 200, 1)       0         \n",
      "=================================================================\n",
      "Total params: 41,689,920\n",
      "Trainable params: 41,686,336\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.676079, acc.: 43.75%] [G loss: 0.659349]\n",
      "1 [D loss: 0.767330, acc.: 81.25%] [G loss: 0.505814]\n",
      "2 [D loss: 1.074994, acc.: 37.50%] [G loss: 0.331815]\n",
      "3 [D loss: 0.646884, acc.: 46.88%] [G loss: 0.196726]\n",
      "4 [D loss: 0.667920, acc.: 50.00%] [G loss: 0.235787]\n",
      "5 [D loss: 0.672891, acc.: 56.25%] [G loss: 0.188262]\n",
      "6 [D loss: 0.750210, acc.: 46.88%] [G loss: 0.202934]\n",
      "7 [D loss: 1.008611, acc.: 53.12%] [G loss: 0.312416]\n",
      "8 [D loss: 0.851814, acc.: 56.25%] [G loss: 0.260124]\n",
      "9 [D loss: 0.962796, acc.: 59.38%] [G loss: 0.800490]\n",
      "10 [D loss: 0.946421, acc.: 65.62%] [G loss: 0.544165]\n",
      "11 [D loss: 1.118199, acc.: 56.25%] [G loss: 0.879939]\n",
      "12 [D loss: 0.524294, acc.: 75.00%] [G loss: 1.398669]\n",
      "13 [D loss: 0.663418, acc.: 71.88%] [G loss: 1.361270]\n",
      "14 [D loss: 0.789817, acc.: 71.88%] [G loss: 1.786401]\n",
      "15 [D loss: 0.926145, acc.: 62.50%] [G loss: 2.675173]\n",
      "16 [D loss: 0.615773, acc.: 78.12%] [G loss: 2.774724]\n",
      "17 [D loss: 1.219764, acc.: 65.62%] [G loss: 2.232646]\n",
      "18 [D loss: 0.903683, acc.: 71.88%] [G loss: 1.918676]\n",
      "19 [D loss: 1.114696, acc.: 62.50%] [G loss: 1.732881]\n",
      "20 [D loss: 2.199044, acc.: 59.38%] [G loss: 2.728812]\n",
      "21 [D loss: 0.867863, acc.: 75.00%] [G loss: 2.090540]\n",
      "22 [D loss: 0.854057, acc.: 68.75%] [G loss: 2.307709]\n",
      "23 [D loss: 0.248506, acc.: 90.62%] [G loss: 2.479967]\n",
      "24 [D loss: 1.926489, acc.: 50.00%] [G loss: 2.548767]\n",
      "25 [D loss: 1.429547, acc.: 50.00%] [G loss: 2.396032]\n",
      "26 [D loss: 0.751402, acc.: 65.62%] [G loss: 3.416722]\n",
      "27 [D loss: 1.448141, acc.: 65.62%] [G loss: 2.353389]\n",
      "28 [D loss: 1.268043, acc.: 62.50%] [G loss: 2.321248]\n",
      "29 [D loss: 1.022458, acc.: 75.00%] [G loss: 4.066222]\n",
      "30 [D loss: 0.600394, acc.: 71.88%] [G loss: 3.773604]\n",
      "31 [D loss: 0.723495, acc.: 71.88%] [G loss: 3.045438]\n",
      "32 [D loss: 0.555496, acc.: 59.38%] [G loss: 2.683095]\n",
      "33 [D loss: 1.018544, acc.: 62.50%] [G loss: 2.671749]\n",
      "34 [D loss: 0.647059, acc.: 71.88%] [G loss: 2.313371]\n",
      "35 [D loss: 0.661498, acc.: 81.25%] [G loss: 2.842639]\n",
      "36 [D loss: 0.715089, acc.: 68.75%] [G loss: 2.428848]\n",
      "37 [D loss: 1.154829, acc.: 62.50%] [G loss: 2.403767]\n",
      "38 [D loss: 0.566582, acc.: 84.38%] [G loss: 2.837081]\n",
      "39 [D loss: 0.542746, acc.: 87.50%] [G loss: 2.971908]\n",
      "40 [D loss: 1.041282, acc.: 65.62%] [G loss: 4.135446]\n",
      "41 [D loss: 2.000179, acc.: 56.25%] [G loss: 2.081904]\n",
      "42 [D loss: 0.761637, acc.: 71.88%] [G loss: 4.761186]\n",
      "43 [D loss: 0.547580, acc.: 71.88%] [G loss: 4.342532]\n",
      "44 [D loss: 0.938325, acc.: 68.75%] [G loss: 2.676526]\n",
      "45 [D loss: 0.421955, acc.: 81.25%] [G loss: 3.732738]\n",
      "46 [D loss: 1.252641, acc.: 68.75%] [G loss: 4.181217]\n",
      "47 [D loss: 1.661173, acc.: 71.88%] [G loss: 3.190763]\n",
      "48 [D loss: 0.775608, acc.: 62.50%] [G loss: 3.258782]\n",
      "49 [D loss: 0.307529, acc.: 84.38%] [G loss: 3.998268]\n",
      "50 [D loss: 1.614685, acc.: 59.38%] [G loss: 2.105313]\n",
      "51 [D loss: 0.878999, acc.: 78.12%] [G loss: 2.709781]\n",
      "52 [D loss: 0.775660, acc.: 71.88%] [G loss: 4.650338]\n",
      "53 [D loss: 0.838454, acc.: 65.62%] [G loss: 4.320126]\n",
      "54 [D loss: 0.231753, acc.: 93.75%] [G loss: 4.438920]\n",
      "55 [D loss: 1.187370, acc.: 65.62%] [G loss: 3.098938]\n",
      "56 [D loss: 1.199261, acc.: 71.88%] [G loss: 2.106162]\n",
      "57 [D loss: 0.805334, acc.: 78.12%] [G loss: 5.353157]\n",
      "58 [D loss: 0.786899, acc.: 68.75%] [G loss: 3.276688]\n",
      "59 [D loss: 1.433984, acc.: 68.75%] [G loss: 2.553884]\n",
      "60 [D loss: 0.745628, acc.: 59.38%] [G loss: 2.179938]\n",
      "61 [D loss: 0.467749, acc.: 84.38%] [G loss: 2.794373]\n",
      "62 [D loss: 0.950258, acc.: 62.50%] [G loss: 3.473014]\n",
      "63 [D loss: 1.891030, acc.: 62.50%] [G loss: 2.290080]\n",
      "64 [D loss: 0.685066, acc.: 75.00%] [G loss: 4.273153]\n",
      "65 [D loss: 0.630227, acc.: 68.75%] [G loss: 2.647638]\n",
      "66 [D loss: 0.698267, acc.: 71.88%] [G loss: 4.440103]\n",
      "67 [D loss: 1.111007, acc.: 59.38%] [G loss: 2.734258]\n",
      "68 [D loss: 0.544348, acc.: 78.12%] [G loss: 2.923285]\n",
      "69 [D loss: 0.721566, acc.: 78.12%] [G loss: 3.181001]\n",
      "70 [D loss: 0.319280, acc.: 90.62%] [G loss: 4.116964]\n",
      "71 [D loss: 1.859081, acc.: 59.38%] [G loss: 2.535572]\n",
      "72 [D loss: 0.181904, acc.: 90.62%] [G loss: 4.197441]\n",
      "73 [D loss: 0.346994, acc.: 84.38%] [G loss: 4.410405]\n",
      "74 [D loss: 0.627470, acc.: 75.00%] [G loss: 2.512804]\n",
      "75 [D loss: 1.521645, acc.: 40.62%] [G loss: 1.552513]\n",
      "76 [D loss: 0.480898, acc.: 75.00%] [G loss: 3.397285]\n",
      "77 [D loss: 0.744362, acc.: 65.62%] [G loss: 2.457546]\n",
      "78 [D loss: 0.659538, acc.: 78.12%] [G loss: 2.510895]\n",
      "79 [D loss: 0.581641, acc.: 81.25%] [G loss: 3.156208]\n",
      "80 [D loss: 0.727815, acc.: 71.88%] [G loss: 3.104209]\n",
      "81 [D loss: 1.184455, acc.: 75.00%] [G loss: 3.453139]\n",
      "82 [D loss: 1.106698, acc.: 87.50%] [G loss: 2.662219]\n",
      "83 [D loss: 0.999897, acc.: 81.25%] [G loss: 2.845366]\n",
      "84 [D loss: 1.138509, acc.: 65.62%] [G loss: 2.847387]\n",
      "85 [D loss: 0.524554, acc.: 87.50%] [G loss: 4.235468]\n",
      "86 [D loss: 1.101415, acc.: 68.75%] [G loss: 2.694679]\n",
      "87 [D loss: 0.357867, acc.: 84.38%] [G loss: 2.526351]\n",
      "88 [D loss: 1.277425, acc.: 62.50%] [G loss: 3.207530]\n",
      "89 [D loss: 1.689475, acc.: 53.12%] [G loss: 2.754956]\n",
      "90 [D loss: 0.335187, acc.: 93.75%] [G loss: 3.425957]\n",
      "91 [D loss: 0.780086, acc.: 75.00%] [G loss: 1.817580]\n",
      "92 [D loss: 0.742567, acc.: 65.62%] [G loss: 3.201277]\n",
      "93 [D loss: 0.331343, acc.: 81.25%] [G loss: 3.556654]\n",
      "94 [D loss: 1.048317, acc.: 78.12%] [G loss: 2.177327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 [D loss: 1.170732, acc.: 65.62%] [G loss: 2.014693]\n",
      "96 [D loss: 0.265237, acc.: 90.62%] [G loss: 2.777845]\n",
      "97 [D loss: 0.388505, acc.: 81.25%] [G loss: 2.660751]\n",
      "98 [D loss: 0.396670, acc.: 93.75%] [G loss: 1.966359]\n",
      "99 [D loss: 0.841982, acc.: 71.88%] [G loss: 3.449025]\n",
      "100 [D loss: 1.016982, acc.: 65.62%] [G loss: 1.939352]\n",
      "101 [D loss: 1.226792, acc.: 65.62%] [G loss: 2.992561]\n",
      "102 [D loss: 0.671807, acc.: 68.75%] [G loss: 4.034755]\n",
      "103 [D loss: 0.607012, acc.: 78.12%] [G loss: 3.165904]\n",
      "104 [D loss: 0.593604, acc.: 71.88%] [G loss: 2.393378]\n",
      "105 [D loss: 0.361383, acc.: 84.38%] [G loss: 4.021498]\n",
      "106 [D loss: 0.997189, acc.: 68.75%] [G loss: 3.029881]\n",
      "107 [D loss: 0.587216, acc.: 71.88%] [G loss: 2.974076]\n",
      "108 [D loss: 0.368687, acc.: 87.50%] [G loss: 3.385552]\n",
      "109 [D loss: 1.486718, acc.: 65.62%] [G loss: 2.107825]\n",
      "110 [D loss: 0.806941, acc.: 75.00%] [G loss: 2.241453]\n",
      "111 [D loss: 0.317666, acc.: 84.38%] [G loss: 2.818464]\n",
      "112 [D loss: 0.600983, acc.: 62.50%] [G loss: 1.942349]\n",
      "113 [D loss: 0.235963, acc.: 87.50%] [G loss: 2.424984]\n",
      "114 [D loss: 0.351868, acc.: 81.25%] [G loss: 2.609064]\n",
      "115 [D loss: 0.716325, acc.: 68.75%] [G loss: 3.317519]\n",
      "116 [D loss: 0.241780, acc.: 87.50%] [G loss: 3.202299]\n",
      "117 [D loss: 1.048773, acc.: 78.12%] [G loss: 2.655403]\n",
      "118 [D loss: 1.007686, acc.: 68.75%] [G loss: 1.935588]\n",
      "119 [D loss: 0.351787, acc.: 90.62%] [G loss: 2.945121]\n",
      "120 [D loss: 0.490514, acc.: 75.00%] [G loss: 2.798859]\n",
      "121 [D loss: 0.494110, acc.: 75.00%] [G loss: 3.138974]\n",
      "122 [D loss: 0.466804, acc.: 84.38%] [G loss: 2.719389]\n",
      "123 [D loss: 0.871967, acc.: 71.88%] [G loss: 3.212388]\n",
      "124 [D loss: 0.713903, acc.: 81.25%] [G loss: 1.913544]\n",
      "125 [D loss: 1.393653, acc.: 62.50%] [G loss: 2.780678]\n",
      "126 [D loss: 0.108915, acc.: 100.00%] [G loss: 4.208821]\n",
      "127 [D loss: 0.637047, acc.: 81.25%] [G loss: 2.590892]\n",
      "128 [D loss: 1.269438, acc.: 59.38%] [G loss: 2.927616]\n",
      "129 [D loss: 0.344918, acc.: 84.38%] [G loss: 2.876662]\n",
      "130 [D loss: 0.339835, acc.: 81.25%] [G loss: 2.891901]\n",
      "131 [D loss: 0.716962, acc.: 65.62%] [G loss: 3.267066]\n",
      "132 [D loss: 0.573321, acc.: 81.25%] [G loss: 3.456903]\n",
      "133 [D loss: 1.082474, acc.: 62.50%] [G loss: 2.846383]\n",
      "134 [D loss: 1.800900, acc.: 28.12%] [G loss: 2.181709]\n",
      "135 [D loss: 0.503158, acc.: 81.25%] [G loss: 2.931348]\n",
      "136 [D loss: 0.619756, acc.: 81.25%] [G loss: 3.394174]\n",
      "137 [D loss: 0.913591, acc.: 78.12%] [G loss: 3.732872]\n",
      "138 [D loss: 0.658145, acc.: 78.12%] [G loss: 3.148213]\n",
      "139 [D loss: 0.903701, acc.: 65.62%] [G loss: 3.833930]\n",
      "140 [D loss: 0.850675, acc.: 65.62%] [G loss: 4.009667]\n",
      "141 [D loss: 0.378186, acc.: 81.25%] [G loss: 4.141008]\n",
      "142 [D loss: 0.387962, acc.: 84.38%] [G loss: 2.057343]\n",
      "143 [D loss: 0.485944, acc.: 84.38%] [G loss: 2.309853]\n",
      "144 [D loss: 1.338282, acc.: 71.88%] [G loss: 2.799378]\n",
      "145 [D loss: 0.450287, acc.: 81.25%] [G loss: 3.285407]\n",
      "146 [D loss: 0.402216, acc.: 81.25%] [G loss: 2.990182]\n",
      "147 [D loss: 0.525015, acc.: 71.88%] [G loss: 4.182015]\n",
      "148 [D loss: 0.514794, acc.: 78.12%] [G loss: 3.947544]\n",
      "149 [D loss: 0.620409, acc.: 68.75%] [G loss: 3.046756]\n",
      "150 [D loss: 0.355563, acc.: 78.12%] [G loss: 3.807330]\n",
      "151 [D loss: 0.526262, acc.: 75.00%] [G loss: 3.637774]\n",
      "152 [D loss: 0.234547, acc.: 87.50%] [G loss: 4.621330]\n",
      "153 [D loss: 0.496879, acc.: 90.62%] [G loss: 3.195141]\n",
      "154 [D loss: 0.238008, acc.: 87.50%] [G loss: 2.963735]\n",
      "155 [D loss: 0.569256, acc.: 71.88%] [G loss: 2.824975]\n",
      "156 [D loss: 0.293534, acc.: 87.50%] [G loss: 4.969085]\n",
      "157 [D loss: 1.212722, acc.: 65.62%] [G loss: 3.567780]\n",
      "158 [D loss: 0.776346, acc.: 87.50%] [G loss: 2.789494]\n",
      "159 [D loss: 0.586564, acc.: 81.25%] [G loss: 3.935628]\n",
      "160 [D loss: 1.395658, acc.: 65.62%] [G loss: 3.223714]\n",
      "161 [D loss: 0.418783, acc.: 75.00%] [G loss: 3.869858]\n",
      "162 [D loss: 0.230384, acc.: 90.62%] [G loss: 2.855127]\n",
      "163 [D loss: 0.263129, acc.: 87.50%] [G loss: 3.934958]\n",
      "164 [D loss: 1.247226, acc.: 71.88%] [G loss: 2.754603]\n",
      "165 [D loss: 1.155741, acc.: 65.62%] [G loss: 3.990726]\n",
      "166 [D loss: 0.801557, acc.: 78.12%] [G loss: 4.272595]\n",
      "167 [D loss: 0.455675, acc.: 84.38%] [G loss: 2.324334]\n",
      "168 [D loss: 0.409131, acc.: 84.38%] [G loss: 4.858405]\n",
      "169 [D loss: 0.551606, acc.: 62.50%] [G loss: 2.556954]\n",
      "170 [D loss: 0.309806, acc.: 84.38%] [G loss: 3.415661]\n",
      "171 [D loss: 0.555275, acc.: 81.25%] [G loss: 5.605456]\n",
      "172 [D loss: 0.768060, acc.: 68.75%] [G loss: 4.105875]\n",
      "173 [D loss: 1.234323, acc.: 68.75%] [G loss: 4.256139]\n",
      "174 [D loss: 0.548665, acc.: 81.25%] [G loss: 4.544546]\n",
      "175 [D loss: 0.061706, acc.: 100.00%] [G loss: 5.090729]\n",
      "176 [D loss: 0.706019, acc.: 81.25%] [G loss: 1.778508]\n",
      "177 [D loss: 0.428688, acc.: 90.62%] [G loss: 2.569331]\n",
      "178 [D loss: 0.563645, acc.: 84.38%] [G loss: 2.327923]\n",
      "179 [D loss: 0.240318, acc.: 90.62%] [G loss: 3.862503]\n",
      "180 [D loss: 0.430493, acc.: 87.50%] [G loss: 3.006775]\n",
      "181 [D loss: 0.424474, acc.: 81.25%] [G loss: 3.715822]\n",
      "182 [D loss: 0.322561, acc.: 87.50%] [G loss: 4.541648]\n",
      "183 [D loss: 0.626774, acc.: 68.75%] [G loss: 5.662048]\n",
      "184 [D loss: 0.397757, acc.: 90.62%] [G loss: 3.989273]\n",
      "185 [D loss: 0.655711, acc.: 93.75%] [G loss: 2.725647]\n",
      "186 [D loss: 0.445466, acc.: 84.38%] [G loss: 2.651806]\n",
      "187 [D loss: 0.147668, acc.: 93.75%] [G loss: 3.209942]\n",
      "188 [D loss: 0.508866, acc.: 75.00%] [G loss: 3.394413]\n",
      "189 [D loss: 0.149617, acc.: 93.75%] [G loss: 3.189149]\n",
      "190 [D loss: 0.189140, acc.: 90.62%] [G loss: 3.086309]\n",
      "191 [D loss: 0.202897, acc.: 90.62%] [G loss: 2.284958]\n",
      "192 [D loss: 0.295462, acc.: 87.50%] [G loss: 4.053055]\n",
      "193 [D loss: 0.509987, acc.: 81.25%] [G loss: 4.801829]\n",
      "194 [D loss: 0.297597, acc.: 84.38%] [G loss: 5.391898]\n",
      "195 [D loss: 0.363453, acc.: 84.38%] [G loss: 4.652499]\n",
      "196 [D loss: 0.469733, acc.: 78.12%] [G loss: 5.166776]\n",
      "197 [D loss: 1.153907, acc.: 71.88%] [G loss: 3.292576]\n",
      "198 [D loss: 0.847660, acc.: 65.62%] [G loss: 5.851561]\n",
      "199 [D loss: 0.333718, acc.: 90.62%] [G loss: 6.818716]\n",
      "200 [D loss: 0.980905, acc.: 71.88%] [G loss: 3.463147]\n",
      "201 [D loss: 0.327568, acc.: 90.62%] [G loss: 3.804161]\n",
      "202 [D loss: 0.524422, acc.: 68.75%] [G loss: 4.001432]\n",
      "203 [D loss: 0.968261, acc.: 68.75%] [G loss: 2.889109]\n",
      "204 [D loss: 0.289472, acc.: 87.50%] [G loss: 4.387659]\n",
      "205 [D loss: 0.259995, acc.: 93.75%] [G loss: 3.054985]\n",
      "206 [D loss: 0.447160, acc.: 81.25%] [G loss: 3.751607]\n",
      "207 [D loss: 0.627407, acc.: 84.38%] [G loss: 3.350602]\n",
      "208 [D loss: 0.796181, acc.: 75.00%] [G loss: 6.175946]\n",
      "209 [D loss: 0.228230, acc.: 90.62%] [G loss: 4.806500]\n",
      "210 [D loss: 1.045444, acc.: 71.88%] [G loss: 3.860456]\n",
      "211 [D loss: 0.178630, acc.: 96.88%] [G loss: 4.010996]\n",
      "212 [D loss: 0.324803, acc.: 90.62%] [G loss: 4.126212]\n",
      "213 [D loss: 0.249458, acc.: 87.50%] [G loss: 4.493184]\n",
      "214 [D loss: 0.737942, acc.: 68.75%] [G loss: 3.060712]\n",
      "215 [D loss: 0.211424, acc.: 90.62%] [G loss: 3.328772]\n",
      "216 [D loss: 0.771740, acc.: 75.00%] [G loss: 4.356091]\n",
      "217 [D loss: 0.530818, acc.: 81.25%] [G loss: 3.521456]\n",
      "218 [D loss: 0.506248, acc.: 81.25%] [G loss: 3.293496]\n",
      "219 [D loss: 0.361388, acc.: 87.50%] [G loss: 3.789881]\n",
      "220 [D loss: 0.178242, acc.: 93.75%] [G loss: 4.331258]\n",
      "221 [D loss: 1.088293, acc.: 53.12%] [G loss: 1.918066]\n",
      "222 [D loss: 0.494635, acc.: 81.25%] [G loss: 4.820576]\n",
      "223 [D loss: 0.581131, acc.: 75.00%] [G loss: 4.379673]\n",
      "224 [D loss: 0.637893, acc.: 78.12%] [G loss: 6.010835]\n",
      "225 [D loss: 0.381836, acc.: 84.38%] [G loss: 5.592236]\n",
      "226 [D loss: 0.406362, acc.: 87.50%] [G loss: 3.388026]\n",
      "227 [D loss: 0.559053, acc.: 78.12%] [G loss: 2.799967]\n",
      "228 [D loss: 0.567710, acc.: 84.38%] [G loss: 5.619430]\n",
      "229 [D loss: 0.559946, acc.: 78.12%] [G loss: 6.297553]\n",
      "230 [D loss: 1.306467, acc.: 62.50%] [G loss: 3.841560]\n",
      "231 [D loss: 1.124920, acc.: 71.88%] [G loss: 4.770382]\n",
      "232 [D loss: 0.784013, acc.: 68.75%] [G loss: 3.489580]\n",
      "233 [D loss: 0.205289, acc.: 93.75%] [G loss: 3.996520]\n",
      "234 [D loss: 1.061591, acc.: 65.62%] [G loss: 5.527594]\n",
      "235 [D loss: 0.247475, acc.: 84.38%] [G loss: 5.516483]\n",
      "236 [D loss: 0.452894, acc.: 84.38%] [G loss: 4.800287]\n",
      "237 [D loss: 0.171771, acc.: 90.62%] [G loss: 3.753109]\n",
      "238 [D loss: 0.211925, acc.: 90.62%] [G loss: 2.026312]\n",
      "239 [D loss: 0.468529, acc.: 84.38%] [G loss: 4.199276]\n",
      "240 [D loss: 0.384076, acc.: 84.38%] [G loss: 5.276087]\n",
      "241 [D loss: 1.223364, acc.: 68.75%] [G loss: 4.184287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 [D loss: 0.813587, acc.: 71.88%] [G loss: 5.842147]\n",
      "243 [D loss: 1.781469, acc.: 62.50%] [G loss: 4.996121]\n",
      "244 [D loss: 0.086832, acc.: 96.88%] [G loss: 5.840962]\n",
      "245 [D loss: 0.699991, acc.: 75.00%] [G loss: 6.696988]\n",
      "246 [D loss: 0.263682, acc.: 87.50%] [G loss: 7.083996]\n",
      "247 [D loss: 0.747895, acc.: 59.38%] [G loss: 4.034898]\n",
      "248 [D loss: 0.392411, acc.: 84.38%] [G loss: 4.008494]\n",
      "249 [D loss: 0.198784, acc.: 90.62%] [G loss: 3.517772]\n",
      "250 [D loss: 0.250086, acc.: 87.50%] [G loss: 4.382744]\n",
      "251 [D loss: 0.599087, acc.: 81.25%] [G loss: 4.174688]\n",
      "252 [D loss: 0.182925, acc.: 93.75%] [G loss: 4.531708]\n",
      "253 [D loss: 0.632795, acc.: 87.50%] [G loss: 4.337148]\n",
      "254 [D loss: 0.843401, acc.: 78.12%] [G loss: 4.969960]\n",
      "255 [D loss: 2.039279, acc.: 56.25%] [G loss: 2.921168]\n",
      "256 [D loss: 0.598585, acc.: 87.50%] [G loss: 5.220844]\n",
      "257 [D loss: 0.206675, acc.: 93.75%] [G loss: 6.337654]\n",
      "258 [D loss: 0.300929, acc.: 90.62%] [G loss: 3.226402]\n",
      "259 [D loss: 0.254023, acc.: 87.50%] [G loss: 4.700965]\n",
      "260 [D loss: 0.322971, acc.: 87.50%] [G loss: 5.549058]\n",
      "261 [D loss: 1.875350, acc.: 53.12%] [G loss: 4.258691]\n",
      "262 [D loss: 0.467526, acc.: 84.38%] [G loss: 4.967759]\n",
      "263 [D loss: 0.447708, acc.: 87.50%] [G loss: 5.444450]\n",
      "264 [D loss: 0.437166, acc.: 75.00%] [G loss: 4.577597]\n",
      "265 [D loss: 0.432780, acc.: 84.38%] [G loss: 4.628052]\n",
      "266 [D loss: 0.496114, acc.: 81.25%] [G loss: 4.747650]\n",
      "267 [D loss: 0.654655, acc.: 84.38%] [G loss: 5.018754]\n",
      "268 [D loss: 0.278474, acc.: 90.62%] [G loss: 3.492225]\n",
      "269 [D loss: 0.224993, acc.: 90.62%] [G loss: 5.311217]\n",
      "270 [D loss: 0.210447, acc.: 90.62%] [G loss: 4.463722]\n",
      "271 [D loss: 0.252346, acc.: 87.50%] [G loss: 4.314935]\n",
      "272 [D loss: 0.489095, acc.: 87.50%] [G loss: 4.337519]\n",
      "273 [D loss: 0.250653, acc.: 81.25%] [G loss: 4.075130]\n",
      "274 [D loss: 0.392586, acc.: 84.38%] [G loss: 3.319751]\n",
      "275 [D loss: 0.318302, acc.: 87.50%] [G loss: 6.425933]\n",
      "276 [D loss: 2.150016, acc.: 46.88%] [G loss: 6.663528]\n",
      "277 [D loss: 0.198493, acc.: 90.62%] [G loss: 4.864967]\n",
      "278 [D loss: 1.027253, acc.: 71.88%] [G loss: 3.851945]\n",
      "279 [D loss: 0.986334, acc.: 75.00%] [G loss: 5.151630]\n",
      "280 [D loss: 0.135297, acc.: 93.75%] [G loss: 5.775545]\n",
      "281 [D loss: 0.267373, acc.: 90.62%] [G loss: 5.693111]\n",
      "282 [D loss: 0.473778, acc.: 81.25%] [G loss: 5.340158]\n",
      "283 [D loss: 0.404179, acc.: 84.38%] [G loss: 7.090940]\n",
      "284 [D loss: 0.517348, acc.: 87.50%] [G loss: 4.585067]\n",
      "285 [D loss: 0.786371, acc.: 68.75%] [G loss: 3.592096]\n",
      "286 [D loss: 0.381150, acc.: 87.50%] [G loss: 5.426804]\n",
      "287 [D loss: 0.746241, acc.: 71.88%] [G loss: 3.237025]\n",
      "288 [D loss: 1.099530, acc.: 71.88%] [G loss: 6.798125]\n",
      "289 [D loss: 0.606599, acc.: 78.12%] [G loss: 4.082174]\n",
      "290 [D loss: 0.265725, acc.: 90.62%] [G loss: 4.116755]\n",
      "291 [D loss: 0.369397, acc.: 81.25%] [G loss: 6.726598]\n",
      "292 [D loss: 0.101205, acc.: 93.75%] [G loss: 5.987055]\n",
      "293 [D loss: 0.333384, acc.: 78.12%] [G loss: 4.583240]\n",
      "294 [D loss: 0.473434, acc.: 87.50%] [G loss: 4.685572]\n",
      "295 [D loss: 0.650272, acc.: 84.38%] [G loss: 5.215573]\n",
      "296 [D loss: 1.583362, acc.: 62.50%] [G loss: 6.359744]\n",
      "297 [D loss: 0.092715, acc.: 96.88%] [G loss: 7.758615]\n",
      "298 [D loss: 1.292231, acc.: 81.25%] [G loss: 4.851924]\n",
      "299 [D loss: 0.369898, acc.: 87.50%] [G loss: 5.511282]\n",
      "300 [D loss: 1.000432, acc.: 75.00%] [G loss: 3.926617]\n",
      "301 [D loss: 0.067563, acc.: 96.88%] [G loss: 5.024101]\n",
      "302 [D loss: 0.217883, acc.: 93.75%] [G loss: 4.082973]\n",
      "303 [D loss: 0.155885, acc.: 93.75%] [G loss: 5.499595]\n",
      "304 [D loss: 0.597940, acc.: 75.00%] [G loss: 5.662663]\n",
      "305 [D loss: 0.532745, acc.: 68.75%] [G loss: 3.809536]\n",
      "306 [D loss: 1.048116, acc.: 81.25%] [G loss: 6.931278]\n",
      "307 [D loss: 0.682083, acc.: 68.75%] [G loss: 4.491808]\n",
      "308 [D loss: 0.132402, acc.: 93.75%] [G loss: 3.982564]\n",
      "309 [D loss: 0.273923, acc.: 87.50%] [G loss: 4.586840]\n",
      "310 [D loss: 0.876466, acc.: 81.25%] [G loss: 4.816654]\n",
      "311 [D loss: 1.281489, acc.: 71.88%] [G loss: 4.391571]\n",
      "312 [D loss: 0.417404, acc.: 87.50%] [G loss: 4.427805]\n",
      "313 [D loss: 0.371726, acc.: 90.62%] [G loss: 3.829571]\n",
      "314 [D loss: 0.074781, acc.: 96.88%] [G loss: 4.527684]\n",
      "315 [D loss: 0.172451, acc.: 90.62%] [G loss: 4.790694]\n",
      "316 [D loss: 0.430762, acc.: 81.25%] [G loss: 4.078817]\n",
      "317 [D loss: 0.513020, acc.: 84.38%] [G loss: 7.716755]\n",
      "318 [D loss: 0.874181, acc.: 78.12%] [G loss: 7.273151]\n",
      "319 [D loss: 0.522930, acc.: 81.25%] [G loss: 5.291656]\n",
      "320 [D loss: 0.144918, acc.: 93.75%] [G loss: 5.265089]\n",
      "321 [D loss: 0.713957, acc.: 59.38%] [G loss: 3.494102]\n",
      "322 [D loss: 0.569408, acc.: 81.25%] [G loss: 5.366177]\n",
      "323 [D loss: 0.067431, acc.: 96.88%] [G loss: 5.938427]\n",
      "324 [D loss: 0.247608, acc.: 84.38%] [G loss: 5.958373]\n",
      "325 [D loss: 0.078335, acc.: 96.88%] [G loss: 4.959220]\n",
      "326 [D loss: 0.049626, acc.: 100.00%] [G loss: 5.397218]\n",
      "327 [D loss: 0.083639, acc.: 100.00%] [G loss: 5.537177]\n",
      "328 [D loss: 0.211419, acc.: 87.50%] [G loss: 4.807814]\n",
      "329 [D loss: 0.460447, acc.: 93.75%] [G loss: 3.943159]\n",
      "330 [D loss: 0.503453, acc.: 81.25%] [G loss: 4.247584]\n",
      "331 [D loss: 0.352932, acc.: 84.38%] [G loss: 5.344033]\n",
      "332 [D loss: 0.102986, acc.: 100.00%] [G loss: 6.245102]\n",
      "333 [D loss: 0.426858, acc.: 87.50%] [G loss: 5.879047]\n",
      "334 [D loss: 0.333051, acc.: 87.50%] [G loss: 4.934820]\n",
      "335 [D loss: 0.103379, acc.: 93.75%] [G loss: 5.349983]\n",
      "336 [D loss: 0.201543, acc.: 90.62%] [G loss: 8.568172]\n",
      "337 [D loss: 0.449777, acc.: 87.50%] [G loss: 6.027878]\n",
      "338 [D loss: 0.243353, acc.: 96.88%] [G loss: 6.209534]\n",
      "339 [D loss: 0.647517, acc.: 84.38%] [G loss: 3.877427]\n",
      "340 [D loss: 0.564814, acc.: 78.12%] [G loss: 4.916544]\n",
      "341 [D loss: 0.138491, acc.: 90.62%] [G loss: 5.063190]\n",
      "342 [D loss: 0.197965, acc.: 90.62%] [G loss: 5.543710]\n",
      "343 [D loss: 0.197363, acc.: 87.50%] [G loss: 6.103559]\n",
      "344 [D loss: 0.095812, acc.: 96.88%] [G loss: 6.153186]\n",
      "345 [D loss: 0.328826, acc.: 84.38%] [G loss: 6.219036]\n",
      "346 [D loss: 0.063290, acc.: 96.88%] [G loss: 6.647617]\n",
      "347 [D loss: 0.773793, acc.: 62.50%] [G loss: 6.499807]\n",
      "348 [D loss: 0.246694, acc.: 87.50%] [G loss: 4.760804]\n",
      "349 [D loss: 0.381574, acc.: 84.38%] [G loss: 5.970582]\n",
      "350 [D loss: 0.329436, acc.: 87.50%] [G loss: 5.824577]\n",
      "351 [D loss: 0.354473, acc.: 87.50%] [G loss: 4.593928]\n",
      "352 [D loss: 0.372336, acc.: 84.38%] [G loss: 6.239802]\n",
      "353 [D loss: 0.068535, acc.: 100.00%] [G loss: 5.799661]\n",
      "354 [D loss: 0.249949, acc.: 84.38%] [G loss: 5.706879]\n",
      "355 [D loss: 0.216047, acc.: 90.62%] [G loss: 3.985631]\n",
      "356 [D loss: 0.144589, acc.: 96.88%] [G loss: 5.667026]\n",
      "357 [D loss: 0.134215, acc.: 96.88%] [G loss: 6.603013]\n",
      "358 [D loss: 0.528877, acc.: 84.38%] [G loss: 4.672424]\n",
      "359 [D loss: 0.293793, acc.: 90.62%] [G loss: 4.978599]\n",
      "360 [D loss: 0.074034, acc.: 100.00%] [G loss: 6.097957]\n",
      "361 [D loss: 0.203500, acc.: 90.62%] [G loss: 4.618242]\n",
      "362 [D loss: 0.198277, acc.: 90.62%] [G loss: 5.614591]\n",
      "363 [D loss: 0.427947, acc.: 84.38%] [G loss: 3.888728]\n",
      "364 [D loss: 0.150228, acc.: 96.88%] [G loss: 5.649168]\n",
      "365 [D loss: 0.149165, acc.: 93.75%] [G loss: 6.312453]\n",
      "366 [D loss: 0.209042, acc.: 90.62%] [G loss: 5.438282]\n",
      "367 [D loss: 0.064130, acc.: 100.00%] [G loss: 5.012156]\n",
      "368 [D loss: 0.267955, acc.: 87.50%] [G loss: 6.655330]\n",
      "369 [D loss: 0.503548, acc.: 78.12%] [G loss: 5.898352]\n",
      "370 [D loss: 0.322963, acc.: 84.38%] [G loss: 4.177717]\n",
      "371 [D loss: 0.088824, acc.: 96.88%] [G loss: 6.277184]\n",
      "372 [D loss: 0.213649, acc.: 90.62%] [G loss: 6.360310]\n",
      "373 [D loss: 0.200857, acc.: 87.50%] [G loss: 5.503261]\n",
      "374 [D loss: 0.245456, acc.: 93.75%] [G loss: 6.413153]\n",
      "375 [D loss: 0.238565, acc.: 96.88%] [G loss: 5.935035]\n",
      "376 [D loss: 0.546542, acc.: 65.62%] [G loss: 5.641224]\n",
      "377 [D loss: 0.164607, acc.: 93.75%] [G loss: 6.036631]\n",
      "378 [D loss: 0.321083, acc.: 90.62%] [G loss: 6.817116]\n",
      "379 [D loss: 0.090319, acc.: 96.88%] [G loss: 6.071140]\n",
      "380 [D loss: 0.045609, acc.: 100.00%] [G loss: 6.687156]\n",
      "381 [D loss: 0.135639, acc.: 93.75%] [G loss: 7.643790]\n",
      "382 [D loss: 0.097147, acc.: 96.88%] [G loss: 6.073849]\n",
      "383 [D loss: 0.427259, acc.: 87.50%] [G loss: 4.860759]\n",
      "384 [D loss: 0.123425, acc.: 93.75%] [G loss: 5.142428]\n",
      "385 [D loss: 0.340936, acc.: 87.50%] [G loss: 4.246184]\n",
      "386 [D loss: 0.105292, acc.: 96.88%] [G loss: 3.828995]\n",
      "387 [D loss: 0.295066, acc.: 84.38%] [G loss: 7.054482]\n",
      "388 [D loss: 0.134673, acc.: 93.75%] [G loss: 7.311858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389 [D loss: 0.216850, acc.: 93.75%] [G loss: 4.832414]\n",
      "390 [D loss: 0.390899, acc.: 81.25%] [G loss: 5.790022]\n",
      "391 [D loss: 0.336733, acc.: 84.38%] [G loss: 7.326101]\n",
      "392 [D loss: 0.167693, acc.: 90.62%] [G loss: 7.147614]\n",
      "393 [D loss: 0.175381, acc.: 90.62%] [G loss: 4.544852]\n",
      "394 [D loss: 0.102740, acc.: 96.88%] [G loss: 5.195890]\n",
      "395 [D loss: 0.167207, acc.: 93.75%] [G loss: 5.149321]\n",
      "396 [D loss: 0.266002, acc.: 90.62%] [G loss: 8.310774]\n",
      "397 [D loss: 0.167436, acc.: 90.62%] [G loss: 5.581613]\n",
      "398 [D loss: 0.130576, acc.: 93.75%] [G loss: 5.263693]\n",
      "399 [D loss: 0.198285, acc.: 90.62%] [G loss: 6.269851]\n",
      "400 [D loss: 0.235138, acc.: 90.62%] [G loss: 5.691998]\n",
      "401 [D loss: 0.077532, acc.: 93.75%] [G loss: 7.422054]\n",
      "402 [D loss: 0.208409, acc.: 87.50%] [G loss: 5.666374]\n",
      "403 [D loss: 0.140116, acc.: 96.88%] [G loss: 5.363342]\n",
      "404 [D loss: 0.035752, acc.: 100.00%] [G loss: 4.572979]\n",
      "405 [D loss: 0.117071, acc.: 96.88%] [G loss: 4.369845]\n",
      "406 [D loss: 0.186148, acc.: 87.50%] [G loss: 6.928912]\n",
      "407 [D loss: 0.343361, acc.: 87.50%] [G loss: 5.811330]\n",
      "408 [D loss: 0.208307, acc.: 90.62%] [G loss: 4.941974]\n",
      "409 [D loss: 0.276633, acc.: 87.50%] [G loss: 5.047969]\n",
      "410 [D loss: 0.044305, acc.: 100.00%] [G loss: 5.708776]\n",
      "411 [D loss: 0.445271, acc.: 90.62%] [G loss: 4.975311]\n",
      "412 [D loss: 0.359571, acc.: 87.50%] [G loss: 5.515613]\n",
      "413 [D loss: 0.226013, acc.: 93.75%] [G loss: 6.828841]\n",
      "414 [D loss: 0.164784, acc.: 93.75%] [G loss: 4.351198]\n",
      "415 [D loss: 0.209157, acc.: 93.75%] [G loss: 4.013954]\n",
      "416 [D loss: 0.107406, acc.: 90.62%] [G loss: 5.802352]\n",
      "417 [D loss: 0.204886, acc.: 93.75%] [G loss: 4.934904]\n",
      "418 [D loss: 0.232118, acc.: 87.50%] [G loss: 6.277825]\n",
      "419 [D loss: 0.172220, acc.: 93.75%] [G loss: 6.487608]\n",
      "420 [D loss: 0.462846, acc.: 84.38%] [G loss: 4.804973]\n",
      "421 [D loss: 0.401607, acc.: 90.62%] [G loss: 5.481797]\n",
      "422 [D loss: 0.586244, acc.: 78.12%] [G loss: 7.953504]\n",
      "423 [D loss: 0.688993, acc.: 81.25%] [G loss: 6.859118]\n",
      "424 [D loss: 0.377315, acc.: 93.75%] [G loss: 8.388980]\n",
      "425 [D loss: 0.306145, acc.: 90.62%] [G loss: 8.819230]\n",
      "426 [D loss: 0.461390, acc.: 81.25%] [G loss: 4.946538]\n",
      "427 [D loss: 0.463167, acc.: 81.25%] [G loss: 7.389784]\n",
      "428 [D loss: 0.119432, acc.: 96.88%] [G loss: 7.743734]\n",
      "429 [D loss: 0.284411, acc.: 87.50%] [G loss: 5.892994]\n",
      "430 [D loss: 0.225262, acc.: 90.62%] [G loss: 6.436893]\n",
      "431 [D loss: 0.102961, acc.: 96.88%] [G loss: 4.407989]\n",
      "432 [D loss: 0.198737, acc.: 90.62%] [G loss: 6.781216]\n",
      "433 [D loss: 0.081337, acc.: 96.88%] [G loss: 6.187632]\n",
      "434 [D loss: 0.100032, acc.: 96.88%] [G loss: 5.468066]\n",
      "435 [D loss: 0.161409, acc.: 93.75%] [G loss: 3.367275]\n",
      "436 [D loss: 0.148798, acc.: 96.88%] [G loss: 4.365571]\n",
      "437 [D loss: 0.097603, acc.: 93.75%] [G loss: 5.547544]\n",
      "438 [D loss: 0.313138, acc.: 87.50%] [G loss: 5.688099]\n",
      "439 [D loss: 0.264005, acc.: 93.75%] [G loss: 6.339565]\n",
      "440 [D loss: 0.157457, acc.: 96.88%] [G loss: 4.945719]\n",
      "441 [D loss: 0.162847, acc.: 93.75%] [G loss: 4.037867]\n",
      "442 [D loss: 0.488481, acc.: 71.88%] [G loss: 4.568747]\n",
      "443 [D loss: 0.565917, acc.: 84.38%] [G loss: 8.535713]\n",
      "444 [D loss: 0.738819, acc.: 87.50%] [G loss: 7.463008]\n",
      "445 [D loss: 0.123579, acc.: 93.75%] [G loss: 5.471863]\n",
      "446 [D loss: 0.065320, acc.: 100.00%] [G loss: 4.752141]\n",
      "447 [D loss: 0.111574, acc.: 96.88%] [G loss: 4.952390]\n",
      "448 [D loss: 0.221790, acc.: 93.75%] [G loss: 5.521669]\n",
      "449 [D loss: 0.047553, acc.: 100.00%] [G loss: 6.514643]\n",
      "450 [D loss: 0.269250, acc.: 87.50%] [G loss: 6.450068]\n",
      "451 [D loss: 0.160709, acc.: 90.62%] [G loss: 5.521690]\n",
      "452 [D loss: 0.112677, acc.: 96.88%] [G loss: 6.349977]\n",
      "453 [D loss: 0.347822, acc.: 87.50%] [G loss: 5.962713]\n",
      "454 [D loss: 0.360741, acc.: 87.50%] [G loss: 5.721958]\n",
      "455 [D loss: 0.332782, acc.: 90.62%] [G loss: 6.145184]\n",
      "456 [D loss: 0.304751, acc.: 87.50%] [G loss: 5.452204]\n",
      "457 [D loss: 0.109694, acc.: 93.75%] [G loss: 7.245664]\n",
      "458 [D loss: 0.240199, acc.: 84.38%] [G loss: 6.248092]\n",
      "459 [D loss: 0.195781, acc.: 93.75%] [G loss: 5.525445]\n",
      "460 [D loss: 0.039558, acc.: 100.00%] [G loss: 6.192156]\n",
      "461 [D loss: 0.118743, acc.: 93.75%] [G loss: 6.337790]\n",
      "462 [D loss: 0.241808, acc.: 90.62%] [G loss: 5.265682]\n",
      "463 [D loss: 0.057724, acc.: 96.88%] [G loss: 6.252622]\n",
      "464 [D loss: 0.284321, acc.: 93.75%] [G loss: 6.715421]\n",
      "465 [D loss: 0.302561, acc.: 87.50%] [G loss: 6.617019]\n",
      "466 [D loss: 0.434509, acc.: 84.38%] [G loss: 5.583146]\n",
      "467 [D loss: 0.266321, acc.: 84.38%] [G loss: 4.840201]\n",
      "468 [D loss: 0.193506, acc.: 96.88%] [G loss: 6.314075]\n",
      "469 [D loss: 0.067975, acc.: 96.88%] [G loss: 6.023513]\n",
      "470 [D loss: 0.111693, acc.: 93.75%] [G loss: 4.785344]\n",
      "471 [D loss: 0.239744, acc.: 81.25%] [G loss: 7.730794]\n",
      "472 [D loss: 0.136468, acc.: 96.88%] [G loss: 6.911962]\n",
      "473 [D loss: 0.281732, acc.: 93.75%] [G loss: 3.868664]\n",
      "474 [D loss: 0.099373, acc.: 100.00%] [G loss: 4.049506]\n",
      "475 [D loss: 0.083816, acc.: 96.88%] [G loss: 4.488121]\n",
      "476 [D loss: 0.157474, acc.: 93.75%] [G loss: 5.187154]\n",
      "477 [D loss: 0.162087, acc.: 96.88%] [G loss: 4.293803]\n",
      "478 [D loss: 0.093504, acc.: 96.88%] [G loss: 5.867968]\n",
      "479 [D loss: 0.129354, acc.: 100.00%] [G loss: 6.022459]\n",
      "480 [D loss: 0.036255, acc.: 100.00%] [G loss: 5.227828]\n",
      "481 [D loss: 0.083774, acc.: 93.75%] [G loss: 6.631709]\n",
      "482 [D loss: 0.243412, acc.: 90.62%] [G loss: 4.528915]\n",
      "483 [D loss: 0.178175, acc.: 87.50%] [G loss: 6.361197]\n",
      "484 [D loss: 0.029584, acc.: 100.00%] [G loss: 7.702662]\n",
      "485 [D loss: 0.085348, acc.: 96.88%] [G loss: 5.249203]\n",
      "486 [D loss: 0.094337, acc.: 93.75%] [G loss: 7.031468]\n",
      "487 [D loss: 0.589630, acc.: 81.25%] [G loss: 5.480546]\n",
      "488 [D loss: 0.070404, acc.: 96.88%] [G loss: 6.033451]\n",
      "489 [D loss: 0.067612, acc.: 100.00%] [G loss: 4.918832]\n",
      "490 [D loss: 0.076412, acc.: 96.88%] [G loss: 4.182244]\n",
      "491 [D loss: 0.118939, acc.: 93.75%] [G loss: 5.096432]\n",
      "492 [D loss: 0.204511, acc.: 90.62%] [G loss: 6.932341]\n",
      "493 [D loss: 0.161615, acc.: 93.75%] [G loss: 4.665850]\n",
      "494 [D loss: 0.040502, acc.: 100.00%] [G loss: 6.762333]\n",
      "495 [D loss: 0.293020, acc.: 93.75%] [G loss: 5.063365]\n",
      "496 [D loss: 0.081805, acc.: 100.00%] [G loss: 5.827108]\n",
      "497 [D loss: 0.078642, acc.: 96.88%] [G loss: 6.219486]\n",
      "498 [D loss: 0.134224, acc.: 96.88%] [G loss: 4.721464]\n",
      "499 [D loss: 0.226158, acc.: 90.62%] [G loss: 5.227482]\n",
      "500 [D loss: 0.021878, acc.: 100.00%] [G loss: 6.466223]\n",
      "501 [D loss: 0.509161, acc.: 78.12%] [G loss: 3.121032]\n",
      "502 [D loss: 0.284984, acc.: 90.62%] [G loss: 5.356541]\n",
      "503 [D loss: 0.317212, acc.: 90.62%] [G loss: 6.481903]\n",
      "504 [D loss: 0.357817, acc.: 93.75%] [G loss: 8.460894]\n",
      "505 [D loss: 0.118204, acc.: 93.75%] [G loss: 8.198375]\n",
      "506 [D loss: 0.441826, acc.: 84.38%] [G loss: 4.386375]\n",
      "507 [D loss: 0.230154, acc.: 90.62%] [G loss: 5.158138]\n",
      "508 [D loss: 0.030439, acc.: 100.00%] [G loss: 6.863825]\n",
      "509 [D loss: 0.164054, acc.: 96.88%] [G loss: 5.670349]\n",
      "510 [D loss: 0.358920, acc.: 87.50%] [G loss: 4.683481]\n",
      "511 [D loss: 0.442596, acc.: 90.62%] [G loss: 5.227280]\n",
      "512 [D loss: 0.044263, acc.: 100.00%] [G loss: 6.823246]\n",
      "513 [D loss: 0.055512, acc.: 100.00%] [G loss: 6.024791]\n",
      "514 [D loss: 0.301076, acc.: 90.62%] [G loss: 6.289168]\n",
      "515 [D loss: 0.117262, acc.: 93.75%] [G loss: 7.923026]\n",
      "516 [D loss: 0.150519, acc.: 96.88%] [G loss: 5.615287]\n",
      "517 [D loss: 0.090911, acc.: 93.75%] [G loss: 5.871516]\n",
      "518 [D loss: 0.470772, acc.: 84.38%] [G loss: 5.452971]\n",
      "519 [D loss: 0.109413, acc.: 96.88%] [G loss: 6.467635]\n",
      "520 [D loss: 0.036202, acc.: 100.00%] [G loss: 6.546632]\n",
      "521 [D loss: 0.249810, acc.: 87.50%] [G loss: 5.377674]\n",
      "522 [D loss: 0.109978, acc.: 96.88%] [G loss: 5.629851]\n",
      "523 [D loss: 0.090341, acc.: 93.75%] [G loss: 6.287949]\n",
      "524 [D loss: 0.029524, acc.: 100.00%] [G loss: 6.117305]\n",
      "525 [D loss: 0.092426, acc.: 96.88%] [G loss: 4.466286]\n",
      "526 [D loss: 0.130441, acc.: 93.75%] [G loss: 5.263514]\n",
      "527 [D loss: 0.058699, acc.: 96.88%] [G loss: 5.766108]\n",
      "528 [D loss: 0.412158, acc.: 90.62%] [G loss: 5.613106]\n",
      "529 [D loss: 0.017756, acc.: 100.00%] [G loss: 5.767674]\n",
      "530 [D loss: 0.104116, acc.: 100.00%] [G loss: 6.708226]\n",
      "531 [D loss: 0.120550, acc.: 96.88%] [G loss: 6.931568]\n",
      "532 [D loss: 0.287211, acc.: 93.75%] [G loss: 5.097501]\n",
      "533 [D loss: 0.154741, acc.: 90.62%] [G loss: 6.081070]\n",
      "534 [D loss: 0.121140, acc.: 93.75%] [G loss: 6.900228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535 [D loss: 0.024463, acc.: 100.00%] [G loss: 6.625327]\n",
      "536 [D loss: 0.303764, acc.: 87.50%] [G loss: 4.675804]\n",
      "537 [D loss: 0.090076, acc.: 96.88%] [G loss: 7.335235]\n",
      "538 [D loss: 0.128857, acc.: 90.62%] [G loss: 5.653393]\n",
      "539 [D loss: 0.062255, acc.: 100.00%] [G loss: 6.256038]\n",
      "540 [D loss: 0.067299, acc.: 96.88%] [G loss: 6.218340]\n",
      "541 [D loss: 0.068634, acc.: 96.88%] [G loss: 6.250668]\n",
      "542 [D loss: 0.210394, acc.: 93.75%] [G loss: 5.853169]\n",
      "543 [D loss: 0.175680, acc.: 96.88%] [G loss: 7.214812]\n",
      "544 [D loss: 0.130521, acc.: 96.88%] [G loss: 5.115304]\n",
      "545 [D loss: 0.164822, acc.: 93.75%] [G loss: 7.476113]\n",
      "546 [D loss: 0.028954, acc.: 100.00%] [G loss: 7.456828]\n",
      "547 [D loss: 0.381727, acc.: 84.38%] [G loss: 4.574229]\n",
      "548 [D loss: 0.061491, acc.: 96.88%] [G loss: 5.657329]\n",
      "549 [D loss: 0.023241, acc.: 100.00%] [G loss: 6.785613]\n",
      "550 [D loss: 0.290298, acc.: 87.50%] [G loss: 6.581758]\n",
      "551 [D loss: 0.062693, acc.: 100.00%] [G loss: 6.598353]\n",
      "552 [D loss: 0.486040, acc.: 75.00%] [G loss: 6.454906]\n",
      "553 [D loss: 0.083985, acc.: 96.88%] [G loss: 6.738688]\n",
      "554 [D loss: 0.431210, acc.: 84.38%] [G loss: 4.895701]\n",
      "555 [D loss: 0.131968, acc.: 93.75%] [G loss: 5.677454]\n",
      "556 [D loss: 0.052334, acc.: 96.88%] [G loss: 7.230875]\n",
      "557 [D loss: 0.083684, acc.: 96.88%] [G loss: 5.321377]\n",
      "558 [D loss: 0.012769, acc.: 100.00%] [G loss: 5.200696]\n",
      "559 [D loss: 0.037165, acc.: 100.00%] [G loss: 4.292126]\n",
      "560 [D loss: 0.042281, acc.: 100.00%] [G loss: 3.864552]\n",
      "561 [D loss: 0.086233, acc.: 96.88%] [G loss: 5.647491]\n",
      "562 [D loss: 0.268949, acc.: 81.25%] [G loss: 4.788859]\n",
      "563 [D loss: 0.026937, acc.: 100.00%] [G loss: 5.536974]\n",
      "564 [D loss: 0.022240, acc.: 100.00%] [G loss: 6.087714]\n",
      "565 [D loss: 0.290734, acc.: 87.50%] [G loss: 6.412773]\n",
      "566 [D loss: 0.033439, acc.: 100.00%] [G loss: 4.591304]\n",
      "567 [D loss: 0.044276, acc.: 100.00%] [G loss: 4.500141]\n",
      "568 [D loss: 0.130094, acc.: 93.75%] [G loss: 6.100632]\n",
      "569 [D loss: 0.039205, acc.: 100.00%] [G loss: 6.192731]\n",
      "570 [D loss: 0.037901, acc.: 100.00%] [G loss: 5.787213]\n",
      "571 [D loss: 0.127347, acc.: 93.75%] [G loss: 4.101583]\n",
      "572 [D loss: 0.096301, acc.: 96.88%] [G loss: 5.835359]\n",
      "573 [D loss: 0.152017, acc.: 93.75%] [G loss: 4.744691]\n",
      "574 [D loss: 0.312417, acc.: 87.50%] [G loss: 7.613747]\n",
      "575 [D loss: 0.060051, acc.: 100.00%] [G loss: 8.933264]\n",
      "576 [D loss: 0.378813, acc.: 93.75%] [G loss: 5.329761]\n",
      "577 [D loss: 0.164373, acc.: 93.75%] [G loss: 5.626282]\n",
      "578 [D loss: 0.245922, acc.: 87.50%] [G loss: 6.902719]\n",
      "579 [D loss: 0.145829, acc.: 90.62%] [G loss: 5.687011]\n",
      "580 [D loss: 0.062612, acc.: 100.00%] [G loss: 4.891184]\n",
      "581 [D loss: 0.052334, acc.: 96.88%] [G loss: 5.555694]\n",
      "582 [D loss: 0.237312, acc.: 87.50%] [G loss: 5.726506]\n",
      "583 [D loss: 0.172255, acc.: 93.75%] [G loss: 7.298021]\n",
      "584 [D loss: 0.330384, acc.: 81.25%] [G loss: 5.374931]\n",
      "585 [D loss: 0.069548, acc.: 96.88%] [G loss: 5.852603]\n",
      "586 [D loss: 0.472235, acc.: 75.00%] [G loss: 5.441273]\n",
      "587 [D loss: 0.067892, acc.: 96.88%] [G loss: 8.401464]\n",
      "588 [D loss: 0.490466, acc.: 78.12%] [G loss: 5.432174]\n",
      "589 [D loss: 0.050566, acc.: 100.00%] [G loss: 6.491073]\n",
      "590 [D loss: 0.082323, acc.: 96.88%] [G loss: 8.018919]\n",
      "591 [D loss: 0.012216, acc.: 100.00%] [G loss: 7.326345]\n",
      "592 [D loss: 0.118421, acc.: 93.75%] [G loss: 4.778209]\n",
      "593 [D loss: 0.189374, acc.: 90.62%] [G loss: 6.348584]\n",
      "594 [D loss: 0.049317, acc.: 100.00%] [G loss: 6.106408]\n",
      "595 [D loss: 0.103493, acc.: 96.88%] [G loss: 4.924640]\n",
      "596 [D loss: 0.139359, acc.: 93.75%] [G loss: 3.779727]\n",
      "597 [D loss: 0.053819, acc.: 96.88%] [G loss: 4.123028]\n",
      "598 [D loss: 0.064175, acc.: 96.88%] [G loss: 5.650016]\n",
      "599 [D loss: 0.142734, acc.: 90.62%] [G loss: 5.657485]\n",
      "600 [D loss: 0.400609, acc.: 84.38%] [G loss: 7.305604]\n",
      "601 [D loss: 0.130962, acc.: 93.75%] [G loss: 5.203784]\n",
      "602 [D loss: 0.164023, acc.: 93.75%] [G loss: 6.041041]\n",
      "603 [D loss: 0.044050, acc.: 96.88%] [G loss: 5.901634]\n",
      "604 [D loss: 0.022035, acc.: 100.00%] [G loss: 6.098866]\n",
      "605 [D loss: 0.304615, acc.: 93.75%] [G loss: 3.809358]\n",
      "606 [D loss: 0.141901, acc.: 93.75%] [G loss: 5.972589]\n",
      "607 [D loss: 0.040707, acc.: 100.00%] [G loss: 6.065241]\n",
      "608 [D loss: 0.266270, acc.: 90.62%] [G loss: 3.803873]\n",
      "609 [D loss: 0.184691, acc.: 93.75%] [G loss: 6.695440]\n",
      "610 [D loss: 0.258616, acc.: 90.62%] [G loss: 4.479552]\n",
      "611 [D loss: 0.133196, acc.: 93.75%] [G loss: 7.822702]\n",
      "612 [D loss: 0.172961, acc.: 96.88%] [G loss: 5.541408]\n",
      "613 [D loss: 0.165847, acc.: 93.75%] [G loss: 5.277322]\n",
      "614 [D loss: 0.034250, acc.: 100.00%] [G loss: 7.243491]\n",
      "615 [D loss: 0.212171, acc.: 87.50%] [G loss: 3.805533]\n",
      "616 [D loss: 0.187035, acc.: 84.38%] [G loss: 6.568757]\n",
      "617 [D loss: 0.128725, acc.: 96.88%] [G loss: 5.807404]\n",
      "618 [D loss: 0.290736, acc.: 84.38%] [G loss: 6.920054]\n",
      "619 [D loss: 0.126789, acc.: 90.62%] [G loss: 6.040177]\n",
      "620 [D loss: 0.111683, acc.: 96.88%] [G loss: 5.229478]\n",
      "621 [D loss: 0.170500, acc.: 90.62%] [G loss: 4.476249]\n",
      "622 [D loss: 0.105107, acc.: 90.62%] [G loss: 7.737145]\n",
      "623 [D loss: 0.045252, acc.: 100.00%] [G loss: 9.072996]\n",
      "624 [D loss: 0.069027, acc.: 96.88%] [G loss: 6.359860]\n",
      "625 [D loss: 0.214464, acc.: 87.50%] [G loss: 6.655602]\n",
      "626 [D loss: 0.192671, acc.: 93.75%] [G loss: 6.215532]\n",
      "627 [D loss: 0.020779, acc.: 100.00%] [G loss: 6.913003]\n",
      "628 [D loss: 0.067668, acc.: 96.88%] [G loss: 6.458282]\n",
      "629 [D loss: 0.065325, acc.: 100.00%] [G loss: 6.797550]\n",
      "630 [D loss: 0.240228, acc.: 87.50%] [G loss: 5.844099]\n",
      "631 [D loss: 0.039592, acc.: 100.00%] [G loss: 6.432499]\n",
      "632 [D loss: 0.325994, acc.: 84.38%] [G loss: 4.925386]\n",
      "633 [D loss: 0.337371, acc.: 84.38%] [G loss: 8.166945]\n",
      "634 [D loss: 0.242417, acc.: 84.38%] [G loss: 6.813362]\n",
      "635 [D loss: 0.023093, acc.: 100.00%] [G loss: 6.407131]\n",
      "636 [D loss: 0.150917, acc.: 93.75%] [G loss: 6.301431]\n",
      "637 [D loss: 0.060760, acc.: 96.88%] [G loss: 6.025778]\n",
      "638 [D loss: 0.213046, acc.: 93.75%] [G loss: 5.041201]\n",
      "639 [D loss: 0.058778, acc.: 96.88%] [G loss: 5.937813]\n",
      "640 [D loss: 0.169307, acc.: 90.62%] [G loss: 7.257408]\n",
      "641 [D loss: 0.253199, acc.: 90.62%] [G loss: 5.267560]\n",
      "642 [D loss: 0.277243, acc.: 90.62%] [G loss: 5.788420]\n",
      "643 [D loss: 0.067069, acc.: 96.88%] [G loss: 8.563032]\n",
      "644 [D loss: 0.558757, acc.: 84.38%] [G loss: 6.180155]\n",
      "645 [D loss: 0.037503, acc.: 100.00%] [G loss: 6.996233]\n",
      "646 [D loss: 0.553701, acc.: 78.12%] [G loss: 5.014023]\n",
      "647 [D loss: 0.019158, acc.: 100.00%] [G loss: 6.984919]\n",
      "648 [D loss: 0.700483, acc.: 71.88%] [G loss: 5.594060]\n",
      "649 [D loss: 0.044413, acc.: 96.88%] [G loss: 6.759934]\n",
      "650 [D loss: 0.196588, acc.: 87.50%] [G loss: 8.153119]\n",
      "651 [D loss: 1.204744, acc.: 75.00%] [G loss: 6.927838]\n",
      "652 [D loss: 0.058647, acc.: 100.00%] [G loss: 7.589032]\n",
      "653 [D loss: 0.054663, acc.: 100.00%] [G loss: 6.367464]\n",
      "654 [D loss: 0.027506, acc.: 100.00%] [G loss: 6.259741]\n",
      "655 [D loss: 0.016603, acc.: 100.00%] [G loss: 6.002047]\n",
      "656 [D loss: 0.127254, acc.: 96.88%] [G loss: 6.425680]\n",
      "657 [D loss: 0.016238, acc.: 100.00%] [G loss: 5.597389]\n",
      "658 [D loss: 0.532888, acc.: 75.00%] [G loss: 3.517647]\n",
      "659 [D loss: 0.221355, acc.: 84.38%] [G loss: 7.518604]\n",
      "660 [D loss: 0.132504, acc.: 96.88%] [G loss: 6.689242]\n",
      "661 [D loss: 0.267116, acc.: 93.75%] [G loss: 4.119696]\n",
      "662 [D loss: 0.354886, acc.: 84.38%] [G loss: 7.779377]\n",
      "663 [D loss: 0.170220, acc.: 90.62%] [G loss: 6.648573]\n",
      "664 [D loss: 0.091873, acc.: 93.75%] [G loss: 6.144379]\n",
      "665 [D loss: 0.210313, acc.: 90.62%] [G loss: 6.966562]\n",
      "666 [D loss: 0.223934, acc.: 96.88%] [G loss: 4.590905]\n",
      "667 [D loss: 0.572598, acc.: 78.12%] [G loss: 5.654253]\n",
      "668 [D loss: 0.086948, acc.: 96.88%] [G loss: 6.252551]\n",
      "669 [D loss: 0.086682, acc.: 96.88%] [G loss: 4.922350]\n",
      "670 [D loss: 0.038470, acc.: 100.00%] [G loss: 5.104666]\n",
      "671 [D loss: 0.258525, acc.: 87.50%] [G loss: 6.395807]\n",
      "672 [D loss: 0.158398, acc.: 93.75%] [G loss: 4.388454]\n",
      "673 [D loss: 0.175167, acc.: 93.75%] [G loss: 5.179483]\n",
      "674 [D loss: 0.037912, acc.: 100.00%] [G loss: 6.868182]\n",
      "675 [D loss: 0.218583, acc.: 93.75%] [G loss: 4.739842]\n",
      "676 [D loss: 0.154274, acc.: 90.62%] [G loss: 3.905015]\n",
      "677 [D loss: 0.046068, acc.: 100.00%] [G loss: 4.002469]\n",
      "678 [D loss: 0.083447, acc.: 96.88%] [G loss: 4.224772]\n",
      "679 [D loss: 0.145914, acc.: 90.62%] [G loss: 6.066677]\n",
      "680 [D loss: 0.246168, acc.: 93.75%] [G loss: 7.353663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681 [D loss: 0.177314, acc.: 93.75%] [G loss: 5.546850]\n",
      "682 [D loss: 0.134855, acc.: 93.75%] [G loss: 4.877928]\n",
      "683 [D loss: 0.067567, acc.: 100.00%] [G loss: 5.544353]\n",
      "684 [D loss: 0.092975, acc.: 96.88%] [G loss: 6.612437]\n",
      "685 [D loss: 0.228921, acc.: 87.50%] [G loss: 4.501318]\n",
      "686 [D loss: 0.054889, acc.: 100.00%] [G loss: 6.144521]\n",
      "687 [D loss: 0.096175, acc.: 96.88%] [G loss: 4.056989]\n",
      "688 [D loss: 0.238210, acc.: 90.62%] [G loss: 4.656323]\n",
      "689 [D loss: 0.162808, acc.: 90.62%] [G loss: 6.590250]\n",
      "690 [D loss: 0.129810, acc.: 93.75%] [G loss: 8.334032]\n",
      "691 [D loss: 0.135337, acc.: 93.75%] [G loss: 5.233853]\n",
      "692 [D loss: 0.308306, acc.: 90.62%] [G loss: 4.836656]\n",
      "693 [D loss: 0.166716, acc.: 93.75%] [G loss: 4.942737]\n",
      "694 [D loss: 0.072371, acc.: 96.88%] [G loss: 6.389097]\n",
      "695 [D loss: 0.058535, acc.: 100.00%] [G loss: 7.200727]\n",
      "696 [D loss: 0.143039, acc.: 93.75%] [G loss: 4.361015]\n",
      "697 [D loss: 0.148211, acc.: 90.62%] [G loss: 5.348376]\n",
      "698 [D loss: 0.020612, acc.: 100.00%] [G loss: 6.089561]\n",
      "699 [D loss: 0.331576, acc.: 93.75%] [G loss: 4.411652]\n",
      "700 [D loss: 0.276471, acc.: 81.25%] [G loss: 4.156379]\n",
      "701 [D loss: 0.064130, acc.: 96.88%] [G loss: 5.868747]\n",
      "702 [D loss: 0.261395, acc.: 90.62%] [G loss: 3.674827]\n",
      "703 [D loss: 0.080360, acc.: 96.88%] [G loss: 5.703070]\n",
      "704 [D loss: 0.009661, acc.: 100.00%] [G loss: 7.002735]\n",
      "705 [D loss: 0.819077, acc.: 71.88%] [G loss: 6.900750]\n",
      "706 [D loss: 0.145940, acc.: 90.62%] [G loss: 7.080696]\n",
      "707 [D loss: 0.115433, acc.: 93.75%] [G loss: 7.361561]\n",
      "708 [D loss: 0.380702, acc.: 87.50%] [G loss: 3.184368]\n",
      "709 [D loss: 0.050699, acc.: 96.88%] [G loss: 5.819828]\n",
      "710 [D loss: 0.098430, acc.: 100.00%] [G loss: 4.737823]\n",
      "711 [D loss: 0.334202, acc.: 84.38%] [G loss: 5.054771]\n",
      "712 [D loss: 0.110286, acc.: 96.88%] [G loss: 6.843247]\n",
      "713 [D loss: 0.167558, acc.: 93.75%] [G loss: 5.439586]\n",
      "714 [D loss: 0.072649, acc.: 96.88%] [G loss: 6.185305]\n",
      "715 [D loss: 0.195049, acc.: 87.50%] [G loss: 3.891954]\n",
      "716 [D loss: 0.114070, acc.: 93.75%] [G loss: 6.984279]\n",
      "717 [D loss: 0.131968, acc.: 93.75%] [G loss: 5.210952]\n",
      "718 [D loss: 0.104311, acc.: 93.75%] [G loss: 5.397492]\n",
      "719 [D loss: 0.028310, acc.: 100.00%] [G loss: 6.219497]\n",
      "720 [D loss: 0.236549, acc.: 90.62%] [G loss: 3.561746]\n",
      "721 [D loss: 0.093452, acc.: 96.88%] [G loss: 5.056030]\n",
      "722 [D loss: 0.242180, acc.: 96.88%] [G loss: 5.294623]\n",
      "723 [D loss: 0.061127, acc.: 100.00%] [G loss: 6.126298]\n",
      "724 [D loss: 0.075599, acc.: 96.88%] [G loss: 5.368354]\n",
      "725 [D loss: 0.214209, acc.: 93.75%] [G loss: 8.421634]\n",
      "726 [D loss: 0.488944, acc.: 87.50%] [G loss: 4.500182]\n",
      "727 [D loss: 0.098064, acc.: 93.75%] [G loss: 3.046596]\n",
      "728 [D loss: 0.178832, acc.: 96.88%] [G loss: 5.476319]\n",
      "729 [D loss: 0.018003, acc.: 100.00%] [G loss: 8.205706]\n",
      "730 [D loss: 0.189847, acc.: 93.75%] [G loss: 5.164498]\n",
      "731 [D loss: 0.031578, acc.: 100.00%] [G loss: 5.206994]\n",
      "732 [D loss: 0.042386, acc.: 100.00%] [G loss: 4.587395]\n",
      "733 [D loss: 0.021641, acc.: 100.00%] [G loss: 3.926005]\n",
      "734 [D loss: 0.040355, acc.: 100.00%] [G loss: 4.415695]\n",
      "735 [D loss: 0.016953, acc.: 100.00%] [G loss: 5.724053]\n",
      "736 [D loss: 0.114446, acc.: 96.88%] [G loss: 5.092347]\n",
      "737 [D loss: 0.195708, acc.: 90.62%] [G loss: 5.098713]\n",
      "738 [D loss: 0.098710, acc.: 93.75%] [G loss: 6.254684]\n",
      "739 [D loss: 0.111375, acc.: 93.75%] [G loss: 5.840516]\n",
      "740 [D loss: 0.088831, acc.: 96.88%] [G loss: 4.782772]\n",
      "741 [D loss: 0.307940, acc.: 93.75%] [G loss: 4.758525]\n",
      "742 [D loss: 0.034610, acc.: 100.00%] [G loss: 5.426993]\n",
      "743 [D loss: 0.046113, acc.: 100.00%] [G loss: 4.697627]\n",
      "744 [D loss: 0.034013, acc.: 100.00%] [G loss: 5.386685]\n",
      "745 [D loss: 0.279093, acc.: 90.62%] [G loss: 3.638764]\n",
      "746 [D loss: 0.092158, acc.: 96.88%] [G loss: 7.218342]\n",
      "747 [D loss: 0.063287, acc.: 100.00%] [G loss: 5.728838]\n",
      "748 [D loss: 0.018762, acc.: 100.00%] [G loss: 4.900471]\n",
      "749 [D loss: 0.284970, acc.: 93.75%] [G loss: 3.923676]\n",
      "750 [D loss: 0.052495, acc.: 100.00%] [G loss: 4.872726]\n",
      "751 [D loss: 0.139902, acc.: 96.88%] [G loss: 5.530355]\n",
      "752 [D loss: 0.084738, acc.: 96.88%] [G loss: 5.599389]\n",
      "753 [D loss: 0.013307, acc.: 100.00%] [G loss: 5.656435]\n",
      "754 [D loss: 0.015449, acc.: 100.00%] [G loss: 5.948056]\n",
      "755 [D loss: 0.247216, acc.: 87.50%] [G loss: 7.110353]\n",
      "756 [D loss: 0.483512, acc.: 87.50%] [G loss: 4.317340]\n",
      "757 [D loss: 0.041629, acc.: 100.00%] [G loss: 3.990412]\n",
      "758 [D loss: 0.216048, acc.: 84.38%] [G loss: 6.736259]\n",
      "759 [D loss: 0.026306, acc.: 100.00%] [G loss: 8.724874]\n",
      "760 [D loss: 0.078442, acc.: 96.88%] [G loss: 6.732541]\n",
      "761 [D loss: 0.438218, acc.: 87.50%] [G loss: 5.375133]\n",
      "762 [D loss: 0.010284, acc.: 100.00%] [G loss: 5.394354]\n",
      "763 [D loss: 0.246747, acc.: 90.62%] [G loss: 6.441387]\n",
      "764 [D loss: 0.106623, acc.: 93.75%] [G loss: 6.143251]\n",
      "765 [D loss: 0.220463, acc.: 93.75%] [G loss: 4.481465]\n",
      "766 [D loss: 0.288169, acc.: 93.75%] [G loss: 4.076968]\n",
      "767 [D loss: 0.035417, acc.: 100.00%] [G loss: 5.831489]\n",
      "768 [D loss: 0.086080, acc.: 96.88%] [G loss: 6.399731]\n",
      "769 [D loss: 0.178051, acc.: 93.75%] [G loss: 4.616788]\n",
      "770 [D loss: 0.019803, acc.: 100.00%] [G loss: 5.630534]\n",
      "771 [D loss: 0.048336, acc.: 96.88%] [G loss: 5.133798]\n",
      "772 [D loss: 0.069033, acc.: 96.88%] [G loss: 6.824424]\n",
      "773 [D loss: 0.069067, acc.: 96.88%] [G loss: 5.975616]\n",
      "774 [D loss: 0.058206, acc.: 96.88%] [G loss: 6.896399]\n",
      "775 [D loss: 0.754733, acc.: 84.38%] [G loss: 3.709926]\n",
      "776 [D loss: 0.171400, acc.: 93.75%] [G loss: 6.813069]\n",
      "777 [D loss: 0.098264, acc.: 96.88%] [G loss: 6.376855]\n",
      "778 [D loss: 0.094945, acc.: 100.00%] [G loss: 6.622240]\n",
      "779 [D loss: 0.231793, acc.: 96.88%] [G loss: 3.513024]\n",
      "780 [D loss: 0.176414, acc.: 93.75%] [G loss: 6.783013]\n",
      "781 [D loss: 0.014932, acc.: 100.00%] [G loss: 8.387297]\n",
      "782 [D loss: 0.529606, acc.: 90.62%] [G loss: 4.169620]\n",
      "783 [D loss: 0.094146, acc.: 100.00%] [G loss: 5.264563]\n",
      "784 [D loss: 0.015818, acc.: 100.00%] [G loss: 6.269114]\n",
      "785 [D loss: 0.199441, acc.: 90.62%] [G loss: 3.465195]\n",
      "786 [D loss: 0.039770, acc.: 100.00%] [G loss: 5.450263]\n",
      "787 [D loss: 0.020454, acc.: 100.00%] [G loss: 5.229525]\n",
      "788 [D loss: 0.051118, acc.: 100.00%] [G loss: 5.220984]\n",
      "789 [D loss: 0.251366, acc.: 93.75%] [G loss: 4.565104]\n",
      "790 [D loss: 0.058816, acc.: 96.88%] [G loss: 5.919929]\n",
      "791 [D loss: 0.115484, acc.: 100.00%] [G loss: 5.265899]\n",
      "792 [D loss: 0.041259, acc.: 100.00%] [G loss: 5.651441]\n",
      "793 [D loss: 0.068292, acc.: 100.00%] [G loss: 5.595843]\n",
      "794 [D loss: 0.294197, acc.: 90.62%] [G loss: 4.034117]\n",
      "795 [D loss: 0.096267, acc.: 93.75%] [G loss: 5.733638]\n",
      "796 [D loss: 0.020123, acc.: 100.00%] [G loss: 8.555544]\n",
      "797 [D loss: 0.208954, acc.: 90.62%] [G loss: 4.642953]\n",
      "798 [D loss: 0.025897, acc.: 100.00%] [G loss: 5.254967]\n",
      "799 [D loss: 0.021042, acc.: 100.00%] [G loss: 5.375804]\n",
      "800 [D loss: 0.049407, acc.: 100.00%] [G loss: 5.400115]\n",
      "801 [D loss: 0.031626, acc.: 100.00%] [G loss: 5.403807]\n",
      "802 [D loss: 0.037300, acc.: 100.00%] [G loss: 4.396262]\n",
      "803 [D loss: 0.082386, acc.: 100.00%] [G loss: 4.906948]\n",
      "804 [D loss: 0.065975, acc.: 100.00%] [G loss: 4.404076]\n",
      "805 [D loss: 0.109220, acc.: 96.88%] [G loss: 5.271505]\n",
      "806 [D loss: 0.262801, acc.: 90.62%] [G loss: 4.660397]\n",
      "807 [D loss: 0.118069, acc.: 93.75%] [G loss: 8.029522]\n",
      "808 [D loss: 0.773295, acc.: 68.75%] [G loss: 8.505269]\n",
      "809 [D loss: 0.430892, acc.: 81.25%] [G loss: 5.779858]\n",
      "810 [D loss: 0.126609, acc.: 96.88%] [G loss: 5.022935]\n",
      "811 [D loss: 0.247575, acc.: 96.88%] [G loss: 5.158040]\n",
      "812 [D loss: 0.314410, acc.: 87.50%] [G loss: 6.275199]\n",
      "813 [D loss: 0.013399, acc.: 100.00%] [G loss: 7.533721]\n",
      "814 [D loss: 0.602380, acc.: 71.88%] [G loss: 4.064339]\n",
      "815 [D loss: 0.120405, acc.: 93.75%] [G loss: 7.918043]\n",
      "816 [D loss: 0.664047, acc.: 78.12%] [G loss: 4.596503]\n",
      "817 [D loss: 0.155769, acc.: 90.62%] [G loss: 7.467114]\n",
      "818 [D loss: 0.161834, acc.: 96.88%] [G loss: 7.264509]\n",
      "819 [D loss: 0.097662, acc.: 96.88%] [G loss: 5.632739]\n",
      "820 [D loss: 0.062593, acc.: 96.88%] [G loss: 5.422422]\n",
      "821 [D loss: 0.138116, acc.: 96.88%] [G loss: 4.818300]\n",
      "822 [D loss: 0.022902, acc.: 100.00%] [G loss: 5.002639]\n",
      "823 [D loss: 0.026825, acc.: 100.00%] [G loss: 5.558024]\n",
      "824 [D loss: 0.065475, acc.: 100.00%] [G loss: 7.179887]\n",
      "825 [D loss: 0.032128, acc.: 100.00%] [G loss: 6.357517]\n",
      "826 [D loss: 0.041047, acc.: 100.00%] [G loss: 5.283636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827 [D loss: 0.052779, acc.: 100.00%] [G loss: 6.027441]\n",
      "828 [D loss: 0.021953, acc.: 100.00%] [G loss: 5.436787]\n",
      "829 [D loss: 0.207061, acc.: 90.62%] [G loss: 4.639098]\n",
      "830 [D loss: 0.053065, acc.: 96.88%] [G loss: 7.490411]\n",
      "831 [D loss: 0.210538, acc.: 90.62%] [G loss: 4.793883]\n",
      "832 [D loss: 0.140359, acc.: 93.75%] [G loss: 5.853099]\n",
      "833 [D loss: 0.021065, acc.: 100.00%] [G loss: 6.819191]\n",
      "834 [D loss: 0.226680, acc.: 90.62%] [G loss: 3.523521]\n",
      "835 [D loss: 0.292067, acc.: 90.62%] [G loss: 5.300566]\n",
      "836 [D loss: 0.037037, acc.: 100.00%] [G loss: 7.545459]\n",
      "837 [D loss: 0.446435, acc.: 75.00%] [G loss: 6.520675]\n",
      "838 [D loss: 0.020691, acc.: 100.00%] [G loss: 7.918078]\n",
      "839 [D loss: 0.201781, acc.: 93.75%] [G loss: 6.373387]\n",
      "840 [D loss: 0.077753, acc.: 93.75%] [G loss: 4.903471]\n",
      "841 [D loss: 0.074709, acc.: 100.00%] [G loss: 4.710515]\n",
      "842 [D loss: 0.099526, acc.: 96.88%] [G loss: 5.002622]\n",
      "843 [D loss: 0.010038, acc.: 100.00%] [G loss: 4.869908]\n",
      "844 [D loss: 0.160263, acc.: 93.75%] [G loss: 5.098705]\n",
      "845 [D loss: 0.015451, acc.: 100.00%] [G loss: 6.296897]\n",
      "846 [D loss: 0.046208, acc.: 100.00%] [G loss: 6.283422]\n",
      "847 [D loss: 0.105778, acc.: 93.75%] [G loss: 3.781391]\n",
      "848 [D loss: 0.126593, acc.: 93.75%] [G loss: 4.828949]\n",
      "849 [D loss: 0.026824, acc.: 100.00%] [G loss: 6.374180]\n",
      "850 [D loss: 0.026657, acc.: 100.00%] [G loss: 5.889052]\n",
      "851 [D loss: 0.425373, acc.: 87.50%] [G loss: 4.558662]\n",
      "852 [D loss: 0.064638, acc.: 100.00%] [G loss: 5.411468]\n",
      "853 [D loss: 0.367333, acc.: 81.25%] [G loss: 5.520407]\n",
      "854 [D loss: 0.021824, acc.: 100.00%] [G loss: 7.546640]\n",
      "855 [D loss: 0.386559, acc.: 84.38%] [G loss: 7.835732]\n",
      "856 [D loss: 0.259156, acc.: 93.75%] [G loss: 4.138021]\n",
      "857 [D loss: 0.177113, acc.: 96.88%] [G loss: 4.689114]\n",
      "858 [D loss: 0.021737, acc.: 100.00%] [G loss: 6.481354]\n",
      "859 [D loss: 0.020116, acc.: 100.00%] [G loss: 5.786016]\n",
      "860 [D loss: 0.127465, acc.: 93.75%] [G loss: 4.718982]\n",
      "861 [D loss: 0.053384, acc.: 100.00%] [G loss: 5.212140]\n",
      "862 [D loss: 0.100089, acc.: 96.88%] [G loss: 6.332093]\n",
      "863 [D loss: 0.591375, acc.: 90.62%] [G loss: 4.550541]\n",
      "864 [D loss: 0.058199, acc.: 100.00%] [G loss: 5.809342]\n",
      "865 [D loss: 0.023541, acc.: 100.00%] [G loss: 6.209056]\n",
      "866 [D loss: 0.048734, acc.: 100.00%] [G loss: 5.405863]\n",
      "867 [D loss: 0.114854, acc.: 93.75%] [G loss: 5.105497]\n",
      "868 [D loss: 0.040438, acc.: 96.88%] [G loss: 6.545081]\n",
      "869 [D loss: 0.129553, acc.: 93.75%] [G loss: 5.367004]\n",
      "870 [D loss: 0.049708, acc.: 100.00%] [G loss: 6.520456]\n",
      "871 [D loss: 0.091950, acc.: 96.88%] [G loss: 6.574249]\n",
      "872 [D loss: 0.029862, acc.: 100.00%] [G loss: 6.260860]\n",
      "873 [D loss: 0.277068, acc.: 93.75%] [G loss: 4.723385]\n",
      "874 [D loss: 0.075315, acc.: 93.75%] [G loss: 6.970028]\n",
      "875 [D loss: 0.166834, acc.: 93.75%] [G loss: 4.986664]\n",
      "876 [D loss: 0.086775, acc.: 96.88%] [G loss: 7.133464]\n",
      "877 [D loss: 0.046509, acc.: 96.88%] [G loss: 6.844774]\n",
      "878 [D loss: 0.688479, acc.: 75.00%] [G loss: 4.299991]\n",
      "879 [D loss: 0.041047, acc.: 100.00%] [G loss: 6.919191]\n",
      "880 [D loss: 0.236870, acc.: 87.50%] [G loss: 6.555803]\n",
      "881 [D loss: 0.149934, acc.: 87.50%] [G loss: 5.640701]\n",
      "882 [D loss: 0.267031, acc.: 90.62%] [G loss: 6.414207]\n",
      "883 [D loss: 0.062372, acc.: 96.88%] [G loss: 6.069635]\n",
      "884 [D loss: 0.168722, acc.: 93.75%] [G loss: 3.735598]\n",
      "885 [D loss: 0.098990, acc.: 96.88%] [G loss: 4.864516]\n",
      "886 [D loss: 0.013655, acc.: 100.00%] [G loss: 6.818238]\n",
      "887 [D loss: 0.708968, acc.: 71.88%] [G loss: 4.774959]\n",
      "888 [D loss: 0.058589, acc.: 100.00%] [G loss: 7.505602]\n",
      "889 [D loss: 0.237723, acc.: 90.62%] [G loss: 4.199733]\n",
      "890 [D loss: 0.048742, acc.: 96.88%] [G loss: 3.517675]\n",
      "891 [D loss: 0.156507, acc.: 93.75%] [G loss: 6.943519]\n",
      "892 [D loss: 0.130795, acc.: 93.75%] [G loss: 4.904946]\n",
      "893 [D loss: 0.040477, acc.: 100.00%] [G loss: 4.764709]\n",
      "894 [D loss: 0.106260, acc.: 96.88%] [G loss: 5.467109]\n",
      "895 [D loss: 0.017021, acc.: 100.00%] [G loss: 6.041502]\n",
      "896 [D loss: 0.033292, acc.: 100.00%] [G loss: 5.678826]\n",
      "897 [D loss: 0.279053, acc.: 90.62%] [G loss: 4.685945]\n",
      "898 [D loss: 0.057478, acc.: 96.88%] [G loss: 6.124865]\n",
      "899 [D loss: 0.026130, acc.: 100.00%] [G loss: 5.635589]\n",
      "900 [D loss: 0.036780, acc.: 100.00%] [G loss: 6.338840]\n",
      "901 [D loss: 0.019886, acc.: 100.00%] [G loss: 6.113913]\n",
      "902 [D loss: 0.103621, acc.: 100.00%] [G loss: 4.099352]\n",
      "903 [D loss: 0.044724, acc.: 100.00%] [G loss: 5.197047]\n",
      "904 [D loss: 0.028148, acc.: 100.00%] [G loss: 5.973483]\n",
      "905 [D loss: 0.206557, acc.: 93.75%] [G loss: 5.530908]\n",
      "906 [D loss: 0.082434, acc.: 96.88%] [G loss: 4.920766]\n",
      "907 [D loss: 0.125661, acc.: 93.75%] [G loss: 5.104439]\n",
      "908 [D loss: 0.027824, acc.: 100.00%] [G loss: 6.445736]\n",
      "909 [D loss: 0.148062, acc.: 96.88%] [G loss: 5.499614]\n",
      "910 [D loss: 0.158822, acc.: 90.62%] [G loss: 5.822196]\n",
      "911 [D loss: 0.070950, acc.: 96.88%] [G loss: 6.138999]\n",
      "912 [D loss: 0.048227, acc.: 96.88%] [G loss: 6.297030]\n",
      "913 [D loss: 0.029840, acc.: 100.00%] [G loss: 6.192026]\n",
      "914 [D loss: 0.028235, acc.: 100.00%] [G loss: 5.394808]\n",
      "915 [D loss: 0.027933, acc.: 100.00%] [G loss: 4.412398]\n",
      "916 [D loss: 0.122633, acc.: 93.75%] [G loss: 6.668768]\n",
      "917 [D loss: 0.037147, acc.: 96.88%] [G loss: 6.296098]\n",
      "918 [D loss: 0.154325, acc.: 90.62%] [G loss: 3.292335]\n",
      "919 [D loss: 0.126756, acc.: 96.88%] [G loss: 5.387238]\n",
      "920 [D loss: 0.008079, acc.: 100.00%] [G loss: 7.429684]\n",
      "921 [D loss: 0.191219, acc.: 87.50%] [G loss: 4.887588]\n",
      "922 [D loss: 0.078749, acc.: 96.88%] [G loss: 5.690167]\n",
      "923 [D loss: 0.046190, acc.: 100.00%] [G loss: 5.738926]\n",
      "924 [D loss: 0.059988, acc.: 100.00%] [G loss: 5.625437]\n",
      "925 [D loss: 0.041717, acc.: 96.88%] [G loss: 5.305351]\n",
      "926 [D loss: 0.111930, acc.: 93.75%] [G loss: 7.688292]\n",
      "927 [D loss: 0.092537, acc.: 96.88%] [G loss: 5.470602]\n",
      "928 [D loss: 0.112399, acc.: 93.75%] [G loss: 5.130769]\n",
      "929 [D loss: 0.075828, acc.: 100.00%] [G loss: 6.430361]\n",
      "930 [D loss: 0.257689, acc.: 87.50%] [G loss: 5.551541]\n",
      "931 [D loss: 0.018904, acc.: 100.00%] [G loss: 5.873210]\n",
      "932 [D loss: 0.218899, acc.: 90.62%] [G loss: 6.299722]\n",
      "933 [D loss: 0.063673, acc.: 96.88%] [G loss: 7.770921]\n",
      "934 [D loss: 0.097752, acc.: 96.88%] [G loss: 5.141675]\n",
      "935 [D loss: 0.150100, acc.: 93.75%] [G loss: 6.253942]\n",
      "936 [D loss: 0.054310, acc.: 100.00%] [G loss: 6.322097]\n",
      "937 [D loss: 0.126544, acc.: 96.88%] [G loss: 4.139026]\n",
      "938 [D loss: 0.069031, acc.: 100.00%] [G loss: 6.292375]\n",
      "939 [D loss: 0.014164, acc.: 100.00%] [G loss: 7.498939]\n",
      "940 [D loss: 0.072117, acc.: 96.88%] [G loss: 4.477181]\n",
      "941 [D loss: 0.062750, acc.: 100.00%] [G loss: 5.561405]\n",
      "942 [D loss: 0.048672, acc.: 96.88%] [G loss: 6.498314]\n",
      "943 [D loss: 0.019182, acc.: 100.00%] [G loss: 6.429059]\n",
      "944 [D loss: 0.025497, acc.: 100.00%] [G loss: 6.000379]\n",
      "945 [D loss: 0.085809, acc.: 96.88%] [G loss: 4.746180]\n",
      "946 [D loss: 0.040666, acc.: 100.00%] [G loss: 5.570778]\n",
      "947 [D loss: 0.027486, acc.: 100.00%] [G loss: 5.995332]\n",
      "948 [D loss: 0.021413, acc.: 100.00%] [G loss: 6.609229]\n",
      "949 [D loss: 0.020901, acc.: 100.00%] [G loss: 5.833239]\n",
      "950 [D loss: 0.075847, acc.: 96.88%] [G loss: 7.740206]\n",
      "951 [D loss: 0.056870, acc.: 96.88%] [G loss: 5.528629]\n",
      "952 [D loss: 0.032982, acc.: 100.00%] [G loss: 5.273659]\n",
      "953 [D loss: 0.064506, acc.: 100.00%] [G loss: 5.024376]\n",
      "954 [D loss: 0.009115, acc.: 100.00%] [G loss: 5.305907]\n",
      "955 [D loss: 0.015699, acc.: 100.00%] [G loss: 5.742219]\n",
      "956 [D loss: 0.031384, acc.: 100.00%] [G loss: 5.093581]\n",
      "957 [D loss: 0.175641, acc.: 93.75%] [G loss: 5.746332]\n",
      "958 [D loss: 0.065779, acc.: 96.88%] [G loss: 6.959231]\n",
      "959 [D loss: 0.048467, acc.: 96.88%] [G loss: 4.381832]\n",
      "960 [D loss: 0.338673, acc.: 78.12%] [G loss: 7.050551]\n",
      "961 [D loss: 0.077571, acc.: 96.88%] [G loss: 7.441545]\n",
      "962 [D loss: 0.314735, acc.: 93.75%] [G loss: 4.903054]\n",
      "963 [D loss: 0.136892, acc.: 96.88%] [G loss: 3.770265]\n",
      "964 [D loss: 0.014992, acc.: 100.00%] [G loss: 4.034412]\n",
      "965 [D loss: 0.044521, acc.: 100.00%] [G loss: 5.822595]\n",
      "966 [D loss: 0.074522, acc.: 96.88%] [G loss: 7.846120]\n",
      "967 [D loss: 0.100500, acc.: 93.75%] [G loss: 6.650308]\n",
      "968 [D loss: 0.405500, acc.: 81.25%] [G loss: 7.510147]\n",
      "969 [D loss: 0.040652, acc.: 96.88%] [G loss: 8.896229]\n",
      "970 [D loss: 0.014194, acc.: 100.00%] [G loss: 9.063518]\n",
      "971 [D loss: 0.387648, acc.: 90.62%] [G loss: 5.809719]\n",
      "972 [D loss: 0.030338, acc.: 100.00%] [G loss: 6.091988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973 [D loss: 0.035877, acc.: 96.88%] [G loss: 5.276383]\n",
      "974 [D loss: 0.130269, acc.: 96.88%] [G loss: 7.365512]\n",
      "975 [D loss: 0.270882, acc.: 96.88%] [G loss: 5.847588]\n",
      "976 [D loss: 0.051894, acc.: 96.88%] [G loss: 7.253478]\n",
      "977 [D loss: 0.013924, acc.: 100.00%] [G loss: 7.302804]\n",
      "978 [D loss: 0.001971, acc.: 100.00%] [G loss: 8.007173]\n",
      "979 [D loss: 0.012679, acc.: 100.00%] [G loss: 7.230328]\n",
      "980 [D loss: 0.221961, acc.: 93.75%] [G loss: 6.495571]\n",
      "981 [D loss: 0.012799, acc.: 100.00%] [G loss: 6.792730]\n",
      "982 [D loss: 0.105595, acc.: 93.75%] [G loss: 5.053909]\n",
      "983 [D loss: 0.139243, acc.: 93.75%] [G loss: 4.874441]\n",
      "984 [D loss: 0.043048, acc.: 100.00%] [G loss: 5.854867]\n",
      "985 [D loss: 0.056930, acc.: 96.88%] [G loss: 6.445759]\n",
      "986 [D loss: 0.005095, acc.: 100.00%] [G loss: 5.810003]\n",
      "987 [D loss: 0.012759, acc.: 100.00%] [G loss: 6.367123]\n",
      "988 [D loss: 0.092958, acc.: 96.88%] [G loss: 4.796904]\n",
      "989 [D loss: 0.077075, acc.: 96.88%] [G loss: 5.492301]\n",
      "990 [D loss: 0.008329, acc.: 100.00%] [G loss: 7.380673]\n",
      "991 [D loss: 0.159028, acc.: 96.88%] [G loss: 6.257874]\n",
      "992 [D loss: 0.047694, acc.: 100.00%] [G loss: 5.468971]\n",
      "993 [D loss: 0.309614, acc.: 90.62%] [G loss: 7.026336]\n",
      "994 [D loss: 0.008694, acc.: 100.00%] [G loss: 8.436146]\n",
      "995 [D loss: 0.043750, acc.: 96.88%] [G loss: 6.255959]\n",
      "996 [D loss: 0.040979, acc.: 96.88%] [G loss: 5.936922]\n",
      "997 [D loss: 0.197396, acc.: 96.88%] [G loss: 4.981327]\n",
      "998 [D loss: 0.173631, acc.: 93.75%] [G loss: 4.235708]\n",
      "999 [D loss: 0.039452, acc.: 100.00%] [G loss: 5.033134]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################################################################\n",
    "\n",
    "#Let us also define our optimizer for easy use later on.\n",
    "#That way if you change your mind, you can change it easily here\n",
    "optimizer = Adam(0.0002, 0.5)  #Learning rate and momentum.\n",
    "\n",
    "# Build and compile the discriminator first. \n",
    "#Generator will be trained as part of the combined model, later. \n",
    "#pick the loss function and the type of metric to keep track.                 \n",
    "#Binary cross entropy as we are doing prediction and it is a better\n",
    "#loss function compared to MSE or other. \n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "#build and compile our Discriminator, pick the loss function\n",
    "\n",
    "#SInce we are only generating (faking) images, let us not track any metrics.\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "##This builds the Generator and defines the input noise. \n",
    "#In a GAN the Generator network takes noise z as an input to produce its images.  \n",
    "z = Input(shape=(100,))   #Our random input to the generator\n",
    "img = generator(z)\n",
    "\n",
    "#This ensures that when we combine our networks we only train the Generator.\n",
    "#While generator training we do not want discriminator weights to be adjusted. \n",
    "#This Doesn't affect the above descriminator training.     \n",
    "discriminator.trainable = False  \n",
    "\n",
    "#This specifies that our Discriminator will take the images generated by our Generator\n",
    "#and true dataset and set its output to a parameter called valid, which will indicate\n",
    "#whether the input is real or not.  \n",
    "valid = discriminator(img)  #Validity check on the generated image\n",
    "\n",
    "\n",
    "#Here we combined the models and also set our loss function and optimizer. \n",
    "#Again, we are only training the generator here. \n",
    "#The ultimate goal here is for the Generator to fool the Discriminator.  \n",
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity\n",
    "\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "train(epochs=1000, batch_size=32, save_interval=50)\n",
    "\n",
    "#Save model for future use to generate fake images\n",
    "#Not tested yet... make sure right model is being saved..\n",
    "#Compare with GAN4\n",
    "\n",
    "generator.save('generator_model.h5')  #Test the model on GAN4_predict...\n",
    "#Change epochs back to 30K\n",
    "                \n",
    "#Epochs dictate the number of backward and forward propagations, the batch_size\n",
    "#indicates the number of training samples per backward/forward propagation, and the\n",
    "#sample_interval specifies after how many epochs we call our sample_image function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
